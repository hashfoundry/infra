# Production Monitoring Implementation Guide

## ğŸ¯ **ĞĞ±Ğ·Ğ¾Ñ€**
ĞŸĞ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ production-ready Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ HashFoundry Infrastructure Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Prometheus + Grafana Stack Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Infrastructure as Code (IaC).

## ğŸ—ï¸ **ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Production Monitoring**

### **ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°:**
```yaml
Production Monitoring Stack:
â”œâ”€â”€ Prometheus Server (Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸ ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
â”œâ”€â”€ Grafana (Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´Ñ‹ Ğ¸ Ğ°Ğ»ĞµÑ€Ñ‚Ñ‹)
â”œâ”€â”€ Node Exporter (Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ÑƒĞ·Ğ»Ğ¾Ğ²)
â”œâ”€â”€ kube-state-metrics (Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Kubernetes)
â”œâ”€â”€ NFS Exporter (Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ NFS storage)
â”œâ”€â”€ Blackbox Exporter (Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸)
â”œâ”€â”€ Loki (log aggregation)
â””â”€â”€ Promtail (log collection)
```

### **ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    External Access                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Grafana UI (grafana.hashfoundry.local)                    â”‚
â”‚  Prometheus UI (prometheus.hashfoundry.local)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 NGINX Ingress Controller                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Monitoring Namespace                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Grafana    â”‚  â”‚ Prometheus  â”‚  â”‚kube-state-  â”‚         â”‚
â”‚  â”‚(Alerts+UI)  â”‚  â”‚   Server    â”‚  â”‚  metrics    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                           â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚    Loki     â”‚  â”‚   Blackbox  â”‚  â”‚NFS Exporter â”‚         â”‚
â”‚  â”‚             â”‚  â”‚  Exporter   â”‚  â”‚             â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Worker Nodes                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚Node Exporterâ”‚  â”‚  Promtail   â”‚  â”‚NFS Exporter â”‚         â”‚
â”‚  â”‚ (DaemonSet) â”‚  â”‚ (DaemonSet) â”‚  â”‚             â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **ĞŸÑ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ IaC:**
- âœ… **GitOps** - Ğ²ÑĞµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Git
- âœ… **ArgoCD ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ** - Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ
- âœ… **Helm Charts** - Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
- âœ… **Ğ’ĞµÑ€ÑĞ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ** - Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹
- âœ… **Declarative** - Ğ¾Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´

---

## ğŸ“‹ **ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ**

### **Ğ˜Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°:**
- âœ… Kubernetes ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ HashFoundry HA (3+ ÑƒĞ·Ğ»Ğ°)
- âœ… ArgoCD Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚
- âœ… NGINX Ingress Controller Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½
- âœ… NFS Provisioner Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
- âœ… Ğ”Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Git Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ

### **Ğ ĞµÑÑƒÑ€ÑÑ‹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°:**
```yaml
ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ:
â”œâ”€â”€ CPU: 2 cores Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾
â”œâ”€â”€ Memory: 4Gi Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾
â”œâ”€â”€ Storage: 50Gi Ğ´Ğ»Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ Ğ»Ğ¾Ğ³Ğ¾Ğ²
â””â”€â”€ Network: Ingress Ğ´Ğ»Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°
```

### **DNS Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸:**
```bash
# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² /etc/hosts Ğ¸Ğ»Ğ¸ DNS
<INGRESS_IP> grafana.hashfoundry.local
<INGRESS_IP> prometheus.hashfoundry.local
```

---

## ğŸš€ **ĞŸĞ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ**

## **Ğ¨Ğ°Ğ³ 1: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°**

### **1.1 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹**
```bash
# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°
mkdir -p ha/k8s/addons/monitoring/{prometheus,grafana,node-exporter,kube-state-metrics,loki,promtail,blackbox-exporter}
```

### **1.2 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ namespace**
```bash
# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ° namespace
cat > ha/k8s/addons/monitoring/namespace.yaml << 'EOF'
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
  labels:
    name: monitoring
    app.kubernetes.io/name: monitoring
EOF
```

### **Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¨Ğ°Ğ³Ğ° 1:**
```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹
ls -la ha/k8s/addons/monitoring/
# Ğ”Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ²ÑĞµ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° namespace Ñ„Ğ°Ğ¹Ğ»Ğ°
cat ha/k8s/addons/monitoring/namespace.yaml
```

---

## **Ğ¨Ğ°Ğ³ 2: Prometheus Server**

### **2.1 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Prometheus Helm Chart**
```bash
# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Chart.yaml
cat > ha/k8s/addons/monitoring/prometheus/Chart.yaml << 'EOF'
apiVersion: v2
name: prometheus
description: Prometheus monitoring server for HashFoundry
type: application
version: 0.1.0
appVersion: "2.45.0"

dependencies:
  - name: prometheus
    version: 25.8.0
    repository: https://prometheus-community.github.io/helm-charts
EOF
```

### **2.2 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ values.yaml**
```bash
cat > ha/k8s/addons/monitoring/prometheus/values.yaml << 'EOF'
prometheus:
  # Prometheus server configuration
  server:
    enabled: true
    image:
      repository: prom/prometheus
      tag: v2.45.0
    
    # Resource limits
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    
    # Persistence
    persistentVolume:
      enabled: true
      size: 20Gi
      storageClass: "do-block-storage"
      accessModes:
        - ReadWriteOnce
    
    # Retention
    retention: "30d"
    
    # Service configuration
    service:
      type: ClusterIP
      port: 9090
    
    # Ingress
    ingress:
      enabled: true
      ingressClassName: nginx
      hosts:
        - host: prometheus.hashfoundry.local
          paths:
            - path: /
              pathType: Prefix
      tls:
        - secretName: prometheus-tls
          hosts:
            - prometheus.hashfoundry.local
    
    # Security context
    securityContext:
      runAsUser: 65534
      runAsGroup: 65534
      fsGroup: 65534
    
    # Anti-affinity for HA
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: prometheus
            topologyKey: kubernetes.io/hostname

  # AlertManager integration
  alertmanager:
    enabled: false  # ĞÑ‚ĞºĞ»ÑÑ‡ĞµĞ½ Ğ¿Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
    
  # Node exporter
  nodeExporter:
    enabled: true
    
  # Kube state metrics
  kubeStateMetrics:
    enabled: true
    
  # Pushgateway
  pushgateway:
    enabled: false

  # Scrape configs
  serverFiles:
    prometheus.yml:
      global:
        scrape_interval: 15s
        evaluation_interval: 15s
      
      rule_files:
        - "/etc/prometheus/rules/*.yml"
      
      alerting:
        alertmanagers:
          - static_configs:
              - targets:
                - alertmanager:9093
      
      scrape_configs:
        # Prometheus itself
        - job_name: 'prometheus'
          static_configs:
            - targets: ['localhost:9090']
        
        # Kubernetes API server
        - job_name: 'kubernetes-apiservers'
          kubernetes_sd_configs:
            - role: endpoints
          scheme: https
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          relabel_configs:
            - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
              action: keep
              regex: default;kubernetes;https
        
        # Kubernetes nodes
        - job_name: 'kubernetes-nodes'
          kubernetes_sd_configs:
            - role: node
          scheme: https
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
        
        # Node exporter
        - job_name: 'node-exporter'
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - source_labels: [__meta_kubernetes_endpoints_name]
              action: keep
              regex: node-exporter
        
        # Kubernetes pods
        - job_name: 'kubernetes-pods'
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: kubernetes_namespace
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: kubernetes_pod_name
        
        # ArgoCD metrics
        - job_name: 'argocd-metrics'
          static_configs:
            - targets: ['argocd-metrics.argocd.svc.cluster.local:8082']
        
        - job_name: 'argocd-server-metrics'
          static_configs:
            - targets: ['argocd-server-metrics.argocd.svc.cluster.local:8083']
        
        - job_name: 'argocd-repo-server-metrics'
          static_configs:
            - targets: ['argocd-repo-server.argocd.svc.cluster.local:8084']
        
        # NGINX Ingress metrics
        - job_name: 'nginx-ingress'
          kubernetes_sd_configs:
            - role: pod
              namespaces:
                names:
                  - ingress-nginx
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
              action: keep
              regex: ingress-nginx
            - source_labels: [__meta_kubernetes_pod_container_port_number]
              action: keep
              regex: "10254"

    # Alert rules
    rules:
      groups:
        - name: kubernetes-alerts
          rules:
            - alert: KubernetesNodeReady
              expr: kube_node_status_condition{condition="Ready",status="true"} == 0
              for: 10m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Node not ready
                description: "Node {{ $labels.node }} has been unready for more than 10 minutes."
            
            - alert: KubernetesPodCrashLooping
              expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes pod crash looping
                description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping."
            
            - alert: KubernetesNodeMemoryPressure
              expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Node memory pressure
                description: "Node {{ $labels.node }} has MemoryPressure condition."
            
            - alert: KubernetesNodeDiskPressure
              expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Node disk pressure
                description: "Node {{ $labels.node }} has DiskPressure condition."
        
        - name: argocd-alerts
          rules:
            - alert: ArgoCDAppNotSynced
              expr: argocd_app_info{sync_status!="Synced"} == 1
              for: 15m
              labels:
                severity: warning
              annotations:
                summary: ArgoCD application not synced
                description: "Application {{ $labels.name }} in project {{ $labels.project }} is not synced."
            
            - alert: ArgoCDAppUnhealthy
              expr: argocd_app_info{health_status!="Healthy"} == 1
              for: 15m
              labels:
                severity: critical
              annotations:
                summary: ArgoCD application unhealthy
                description: "Application {{ $labels.name }} in project {{ $labels.project }} is unhealthy."
EOF
```

### **2.3 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Makefile**
```bash
cat > ha/k8s/addons/monitoring/prometheus/Makefile << 'EOF'
.PHONY: install uninstall upgrade status

NAMESPACE = monitoring
RELEASE_NAME = prometheus

install:
	helm dependency update
	helm upgrade --install $(RELEASE_NAME) . \
		--namespace $(NAMESPACE) \
		--create-namespace \
		--values values.yaml \
		--wait

uninstall:
	helm uninstall $(RELEASE_NAME) --namespace $(NAMESPACE)

upgrade:
	helm dependency update
	helm upgrade $(RELEASE_NAME) . \
		--namespace $(NAMESPACE) \
		--values values.yaml \
		--wait

status:
	helm status $(RELEASE_NAME) --namespace $(NAMESPACE)
	kubectl get pods -n $(NAMESPACE) -l app.kubernetes.io/name=prometheus

logs:
	kubectl logs -n $(NAMESPACE) -l app.kubernetes.io/name=prometheus --tail=100 -f
EOF
```

### **Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¨Ğ°Ğ³Ğ° 2:**
```bash
# ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ² Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Prometheus
cd ha/k8s/addons/monitoring/prometheus

# ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
helm dependency update

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ²
helm template prometheus . --values values.yaml --namespace monitoring

# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Prometheus
make install

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°
make status

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ğ´Ğ¾Ğ²
kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° PVC
kubectl get pvc -n monitoring

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞµÑ€Ğ²Ğ¸ÑĞ°
kubectl get svc -n monitoring

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ingress
kubectl get ingress -n monitoring

# Ğ¢ĞµÑÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ (port-forward)
kubectl port-forward -n monitoring svc/prometheus-server 9090:80
# ĞÑ‚ĞºÑ€Ñ‹Ñ‚ÑŒ http://localhost:9090

cd ../../../../
```

---

## **Ğ¨Ğ°Ğ³ 3: Grafana**

### **3.1 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Grafana Helm Chart**
```bash
cat > ha/k8s/addons/monitoring/grafana/Chart.yaml << 'EOF'
apiVersion: v2
name: grafana
description: Grafana dashboard for HashFoundry monitoring
type: application
version: 0.1.0
appVersion: "10.2.0"

dependencies:
  - name: grafana
    version: 7.0.8
    repository: https://grafana.github.io/helm-charts
EOF
```

### **3.2 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ values.yaml**
```bash
cat > ha/k8s/addons/monitoring/grafana/values.yaml << 'EOF'
grafana:
  # Image configuration
  image:
    repository: grafana/grafana
    tag: "10.2.0"
  
  # Admin credentials
  adminUser: admin
  adminPassword: admin
  
  # Resource limits
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 250m
      memory: 512Mi
  
  # Persistence
  persistence:
    enabled: true
    size: 10Gi
    storageClassName: "nfs-client"
    accessModes:
      - ReadWriteMany
  
  # Service configuration
  service:
    type: ClusterIP
    port: 80
  
  # Ingress
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - host: grafana.hashfoundry.local
        paths:
          - path: /
            pathType: Prefix
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.hashfoundry.local
  
  # Security context
  # UID/GID 472 - Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ grafana
  # 
  # ĞŸĞ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ UID 472:
  # - Ğ­Ñ‚Ğ¾ ĞĞ• ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾, Ğ° Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ UID Ğ´Ğ»Ñ Grafana
  # - ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½ Ğ² Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Dockerfile Grafana: https://github.com/grafana/grafana/blob/main/Dockerfile
  # - Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ°Ñ… grafana/grafana
  # - Ğ¡Ğ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°Ğ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ² (non-root user)
  # - ĞĞ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸
  # 
  # Ğ—Ğ°Ñ‡ĞµĞ¼ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ 472:
  # - Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¾Ğ¹ Grafana ĞºĞ°Ğº ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€
  # - Ğ˜Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ (Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ < 1000)
  # - Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¸Ğ½ÑÑ‚Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ¹ Grafana Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ°Ñ…
  # - ĞŸĞ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ volume permissions
  securityContext:
    runAsUser: 472      # ĞÑ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ UID Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ grafana (Ğ½Ğµ root Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸)
    runAsGroup: 472     # ĞÑ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ° grafana Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ² Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°
    fsGroup: 472        # ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ NFS - ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ° Ğ½Ğ° mounted volumes
  
  # Grafana configuration
  grafana.ini:
    server:
      root_url: https://grafana.hashfoundry.local
    security:
      admin_user: admin
      admin_password: admin
    auth.anonymous:
      enabled: false
    analytics:
      check_for_updates: false
      reporting_enabled: false
    log:
      mode: console
      level: info
  
  # Datasources
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://prometheus-server.monitoring.svc.cluster.local
          access: proxy
          isDefault: true
          editable: true
        - name: Loki
          type: loki
          url: http://loki.monitoring.svc.cluster.local:3100
          access: proxy
          editable: true
  
  # Dashboard providers
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
        - name: 'kubernetes'
          orgId: 1
          folder: 'Kubernetes'
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/kubernetes
        - name: 'argocd'
          orgId: 1
          folder: 'ArgoCD'
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/argocd
  
  # Dashboards
  dashboards:
    default:
      # Infrastructure Overview Dashboard
      infrastructure-overview:
        gnetId: 7249
        revision: 1
        datasource: Prometheus
      
      # Node Exporter Dashboard
      node-exporter:
        gnetId: 1860
        revision: 31
        datasource: Prometheus
    
    kubernetes:
      # Kubernetes Cluster Monitoring
      kubernetes-cluster:
        gnetId: 7249
        revision: 1
        datasource: Prometheus
      
      # Kubernetes Pod Monitoring
      kubernetes-pods:
        gnetId: 6417
        revision: 1
        datasource: Prometheus
      
      # Kubernetes Deployment
      kubernetes-deployment:
        gnetId: 8588
        revision: 1
        datasource: Prometheus
    
    argocd:
      # ArgoCD Dashboard
      argocd-overview:
        gnetId: 14584
        revision: 1
        datasource: Prometheus
  
  # Plugins
  plugins:
    - grafana-piechart-panel
    - grafana-worldmap-panel
    - grafana-clock-panel
  
  # SMTP configuration (optional)
  smtp:
    enabled: false
  
  # Anti-affinity for HA
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: grafana
          topologyKey: kubernetes.io/hostname
EOF
```

### **3.3 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Makefile**
```bash
cat > ha/k8s/addons/monitoring/grafana/Makefile << 'EOF'
.PHONY: install uninstall upgrade status

NAMESPACE = monitoring
RELEASE_NAME = grafana

install:
	helm dependency update
	helm upgrade --install $(RELEASE_NAME) . \
		--namespace $(NAMESPACE) \
		--create-namespace \
		--values values.yaml \
		--wait

uninstall:
	helm uninstall $(RELEASE_NAME) --namespace $(NAMESPACE)

upgrade:
	helm dependency update
	helm upgrade $(RELEASE_NAME) . \
		--namespace $(NAMESPACE) \
		--values values.yaml \
		--wait

status:
	helm status $(RELEASE_NAME) --namespace $(NAMESPACE)
	kubectl get pods -n $(NAMESPACE) -l app.kubernetes.io/name=grafana

logs:
	kubectl logs -n $(NAMESPACE) -l app.kubernetes.io/name=grafana --tail=100 -f

password:
	@echo "Grafana admin password:"
	@kubectl get secret --namespace $(NAMESPACE) grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
EOF
```

### **Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¨Ğ°Ğ³Ğ° 3:**
```bash
# ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ² Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Grafana
cd ha/k8s/addons/monitoring/grafana

# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Grafana
make install

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°
make status

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ°Ñ€Ğ¾Ğ»Ñ
make password

# Ğ¢ĞµÑÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ (port-forward)
kubectl port-forward -n monitoring svc/grafana 3000:80
# ĞÑ‚ĞºÑ€Ñ‹Ñ‚ÑŒ http://localhost:3000
# Ğ›Ğ¾Ğ³Ğ¸Ğ½: admin, ĞŸĞ°Ñ€Ğ¾Ğ»ÑŒ: admin

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° datasource Prometheus
# Ğ’ Grafana: Configuration -> Data Sources -> Prometheus
# URL Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ: http://prometheus-server.monitoring.svc.cluster.local

cd ../../../../
```

---

## **Ğ¨Ğ°Ğ³ 4: Grafana Alerting Configuration**

### **4.1 ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Grafana Ğ´Ğ»Ñ Ğ°Ğ»ĞµÑ€Ñ‚Ğ¾Ğ²**
```bash
# ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ values.yaml Ğ´Ğ»Ñ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ»ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Grafana
cat >> ha/k8s/addons/monitoring/grafana/values.yaml << 'EOF'

  # Grafana Alerting configuration
  alerting:
    enabled: true
    # Contact points for notifications
    contactPoints:
      - name: email-alerts
        type: email
        settings:
          addresses: admin@hashfoundry.local
          subject: "ğŸš¨ Grafana Alert: {{ .GroupLabels.alertname }}"
          message: |
            {{ range .Alerts }}
            Alert: {{ .Annotations.summary }}
            Description: {{ .Annotations.description }}
            Status: {{ .Status }}
            {{ end }}
      
      - name: slack-alerts
        type: slack
        settings:
          url: YOUR_SLACK_WEBHOOK_URL
          channel: "#alerts"
          title: "ğŸš¨ Grafana Alert"
          text: |
            {{ range .Alerts }}
            Alert: {{ .Annotations.summary }}
            Description: {{ .Annotations.description }}
            {{ end }}
    
    # Notification policies
    policies:
      - receiver: email-alerts
        group_by: ['alertname']
        group_wait: 10s
        group_interval: 5m
        repeat_interval: 12h
        matchers:
          - severity = critical
      
      - receiver: slack-alerts
        group_by: ['alertname']
        group_wait: 10s
        group_interval: 5m
        repeat_interval: 1h
        matchers:
          - severity = warning

  # Alert rules (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· UI)
  alertRules:
    - name: kubernetes-alerts
      folder: Kubernetes
      rules:
        - uid: node-down
          title: Node Down
          condition: A
          data:
            - refId: A
              queryType: prometheus
              model:
                expr: up{job="kubernetes-nodes"} == 0
                interval: 1m
          noDataState: NoData
          execErrState: Alerting
          for: 5m
          annotations:
            summary: "Node {{ $labels.instance }} is down"
            description: "Node {{ $labels.instance }} has been down for more than 5 minutes"
          labels:
            severity: critical
        
        - uid: high-cpu
          title: High CPU Usage
          condition: A
          data:
            - refId: A
              queryType: prometheus
              model:
                expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
                interval: 1m
          noDataState: NoData
          execErrState: Alerting
          for: 5m
          annotations:
            summary: "High CPU usage on {{ $labels.instance }}"
            description: "CPU usage is above 80% for more than 5 minutes"
          labels:
            severity: warning
        
        - uid: high-memory
          title: High Memory Usage
          condition: A
          data:
            - refId: A
              queryType: prometheus
              model:
                expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
                interval: 1m
          noDataState: NoData
          execErrState: Alerting
          for: 5m
          annotations:
            summary: "High memory usage on {{ $labels.instance }}"
            description: "Memory usage is above 90% for more than 5 minutes"
          labels:
            severity: critical
        
        - uid: disk-space-low
          title: Low Disk Space
          condition: A
          data:
            - refId: A
              queryType: prometheus
              model:
                expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
                interval: 1m
          noDataState: NoData
          execErrState: Alerting
          for: 5m
          annotations:
            summary: "Low disk space on {{ $labels.instance }}"
            description: "Disk usage is above 85% on {{ $labels.mountpoint }}"
          labels:
            severity: warning

    - name: argocd-alerts
      folder: ArgoCD
      rules:
        - uid: argocd-app-not-synced
          title: ArgoCD Application Not Synced
          condition: A
          data:
            - refId: A
              queryType: prometheus
              model:
                expr: argocd_app_info{sync_status!="Synced"} == 1
                interval: 1m
          noDataState: NoData
          execErrState: Alerting
          for: 15m
          annotations:
            summary: "ArgoCD application {{ $labels.name }} not synced"
            description: "Application {{ $labels.name }} in project {{ $labels.project }} is not synced for more than 15 minutes"
          labels:
            severity: warning
        
        - uid: argocd-app-unhealthy
          title: ArgoCD Application Unhealthy
          condition: A
          data:
            - refId: A
              queryType: prometheus
              model:
                expr: argocd_app_info{health_status!="Healthy"} == 1
                interval: 1m
          noDataState: NoData
          execErrState: Alerting
          for: 10m
          annotations:
            summary: "ArgoCD application {{ $labels.name }} unhealthy"
            description: "Application {{ $labels.name }} in project {{ $labels.project }} is unhealthy"
          labels:
            severity: critical
EOF
```

### **4.2 Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Grafana Alerting**
```bash
# ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ² Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Grafana
cd ha/k8s/addons/monitoring/grafana

# ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Grafana Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ°Ğ»ĞµÑ€Ñ‚Ğ¾Ğ²
make upgrade

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°
make status

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ°Ğ»ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Grafana UI
kubectl port-forward -n monitoring svc/grafana 3000:80
# ĞÑ‚ĞºÑ€Ñ‹Ñ‚ÑŒ http://localhost:3000
# ĞŸĞµÑ€ĞµĞ¹Ñ‚Ğ¸ Ğ² Alerting -> Alert Rules

cd ../../../../
```

---

## **Ğ¨Ğ°Ğ³ 5: NFS Exporter**

### **5.1 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ NFS Exporter**
```bash
cat > ha/k8s/addons/monitoring/nfs-exporter/Chart.yaml << 'EOF'
apiVersion: v2
name: nfs-exporter
description: NFS Server metrics exporter for HashFoundry
type: application
version: 0.1.0
appVersion: "1.0.0"
EOF
```

### **5.2 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ values.yaml**
```bash
cat > ha/k8s/addons/monitoring/nfs-exporter/values.yaml << 'EOF'
# NFS Exporter configuration
nfsExporter:
  image:
    repository: kvaps/nfs-server-exporter
    tag: latest
    pullPolicy: IfNotPresent
  
  # Resource limits
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi
  
  # Service configuration
  service:
    type: ClusterIP
    port: 9662
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "9662"
      prometheus.io/path: "/metrics"
  
  # Node selector to run on NFS server node
  nodeSelector:
    nfs-server: "true"
EOF
```

### **5.3 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ templates**
```bash
mkdir -p ha/k8s/addons/monitoring/nfs-exporter/templates

cat > ha/k8s/addons/monitoring/nfs-exporter/templates/deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-exporter
  namespace: monitoring
  labels:
    app.kubernetes.io/name: nfs-exporter
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: nfs-exporter
  template:
    metadata:
      labels:
        app.kubernetes.io/name: nfs-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9662"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: nfs-exporter
        image: {{ .Values.nfsExporter.image.repository }}:{{ .Values.nfsExporter.image.tag }}
        imagePullPolicy: {{ .Values.nfsExporter.image.pullPolicy }}
        ports:
        - containerPort: 9662
          name: metrics
        resources:
          {{- toYaml .Values.nfsExporter.resources | nindent 10 }}
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: nfs-exports
          mountPath: /exports
          readOnly: true
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: nfs-exports
        persistentVolumeClaim:
          claimName: nfs-provisioner-server-pvc
      nodeSelector:
        {{- toYaml .Values.nfsExporter.nodeSelector | nindent 8 }}
EOF

cat > ha/k8s/addons/monitoring/nfs-exporter/templates/service.yaml << 'EOF'
apiVersion: v1
kind: Service
metadata:
  name: nfs-exporter
  namespace: monitoring
  labels:
    app.kubernetes.io/name: nfs-exporter
  annotations:
    {{- toYaml .Values.nfsExporter.service.annotations | nindent 4 }}
spec:
  type: {{ .Values.nfsExporter.service.type }}
  ports:
  - port: {{ .Values.nfsExporter.service.port }}
    targetPort: metrics
    protocol: TCP
    name: metrics
  selector:
    app.kubernetes.io/name: nfs-exporter
EOF
```

### **5.4 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Makefile**
```bash
cat > ha/k8s/addons/monitoring/nfs-exporter/Makefile << 'EOF'
.PHONY: install uninstall upgrade status

NAMESPACE = monitoring
RELEASE_NAME = nfs-exporter

install:
	helm upgrade --install $(RELEASE_NAME) . \
		--namespace $(NAMESPACE) \
		--create-namespace \
		--values values.yaml \
		--wait

uninstall:
	helm uninstall $(RELEASE_NAME) --namespace $(NAMESPACE)

upgrade:
	helm upgrade $(RELEASE_NAME) . \
		--namespace $(NAMESPACE) \
		--values values.yaml \
		--wait

status:
	helm status $(RELEASE_NAME) --namespace $(NAMESPACE)
	kubectl get pods -n $(NAMESPACE) -l app.kubernetes.io/name=nfs-exporter

logs:
	kubectl logs -n $(NAMESPACE) -l app.kubernetes.io/name=nfs-exporter --tail=100 -f
EOF
```

### **Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¨Ğ°Ğ³Ğ° 5:**
```bash
# ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ² Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ NFS Exporter
cd ha/k8s/addons/monitoring/nfs-exporter

# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° NFS Exporter
make install

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°
make status

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº
kubectl port-forward -n monitoring svc/nfs-exporter 9662:9662
# ĞÑ‚ĞºÑ€Ñ‹Ñ‚ÑŒ http://localhost:9662/metrics

cd ../../../../
```

---

## **Ğ¨Ğ°Ğ³ 6: ArgoCD Integration**

### **6.1 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ArgoCD Application Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°**
```bash
cat > ha/k8s/addons/argo-cd-apps/templates/monitoring-applications.yaml << 'EOF'
# Prometheus Application
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: prometheus
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  source:
    repoURL: {{ .Values.spec.source.repoURL }}
    targetRevision: {{ .Values.spec.source.targetRevision }}
    path: ha/k8s/addons/monitoring/prometheus
  destination:
    server: {{ .Values.spec.destination.server }}
    namespace: monitoring
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground
      - PruneLast=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m

---
# Grafana Application
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: grafana
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  source:
    repoURL: {{ .Values.spec.source.repoURL }}
    targetRevision: {{ .Values.spec.source.targetRevision }}
    path: ha/k8s/addons/monitoring/grafana
  destination:
    server: {{ .Values.spec.destination.server }}
    namespace: monitoring
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground
      - PruneLast=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m

---
# NFS Exporter Application
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: nfs-exporter
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  source:
    repoURL: {{ .Values.spec.source.repoURL }}
    targetRevision: {{ .Values.spec.source.targetRevision }}
    path: ha/k8s/addons/monitoring/nfs-exporter
  destination:
    server: {{ .Values.spec.destination.server }}
    namespace: monitoring
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground
      - PruneLast=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF
```

### **Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¨Ğ°Ğ³Ğ° 6:**
```bash
# ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ArgoCD Apps
cd ha/k8s/addons/argo-cd-apps
helm upgrade argo-cd-apps . -n argocd -f values.yaml

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ArgoCD
kubectl get applications -n argocd | grep -E "(prometheus|grafana|nfs-exporter)"

# Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹
kubectl patch application prometheus -n argocd --type merge -p='{"operation":{"sync":{}}}'
kubectl patch application grafana -n argocd --type merge -p='{"operation":{"sync":{}}}'
kubectl patch application nfs-exporter -n argocd --type merge -p='{"operation":{"sync":{}}}'

cd ../../../
```

---

## **Ğ¨Ğ°Ğ³ 7: ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° DNS Ğ¸ Ingress**

### **7.1 ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ /etc/hosts**
```bash
# ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ IP Ğ°Ğ´Ñ€ĞµÑĞ° Ingress Controller
INGRESS_IP=$(kubectl get svc -n ingress-nginx nginx-ingress-ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

echo "Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² /etc/hosts:"
echo "$INGRESS_IP grafana.hashfoundry.local"
echo "$INGRESS_IP prometheus.hashfoundry.local"
echo "$INGRESS_IP alerts.hashfoundry.local"
```

### **7.2 ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ingress**
```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²ÑĞµÑ… Ingress Ğ² monitoring namespace
kubectl get ingress -n monitoring

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· curl
curl -k -H "Host: grafana.hashfoundry.local" https://$INGRESS_IP/
curl -k -H "Host: prometheus.hashfoundry.local" https://$INGRESS_IP/
curl -k -H "Host: alerts.hashfoundry.local" https://$INGRESS_IP/
```

---

## **Ğ¨Ğ°Ğ³ 8: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ**

### **8.1 ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²**
```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ´Ğ¾Ğ² Ğ² monitoring namespace
kubectl get pods -n monitoring

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²ÑĞµÑ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²
kubectl get svc -n monitoring

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²ÑĞµÑ… PVC
kubectl get pvc -n monitoring

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²ÑĞµÑ… Ingress
kubectl get ingress -n monitoring
```

### **8.2 Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ**

#### **Prometheus:**
```bash
# Port-forward Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°
kubectl port-forward -n monitoring svc/prometheus-server 9090:80 &

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° targets
curl http://localhost:9090/api/v1/targets

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº
curl http://localhost:9090/api/v1/query?query=up

# ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° port-forward
pkill -f "kubectl port-forward.*prometheus"
```

#### **Grafana:**
```bash
# Port-forward Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°
kubectl port-forward -n monitoring svc/grafana 3000:80 &

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸
curl http://localhost:3000/api/health

# ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° port-forward
pkill -f "kubectl port-forward.*grafana"
```

#### **Grafana Alerting:**
```bash
# Port-forward Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Grafana
kubectl port-forward -n monitoring svc/grafana 3000:80 &

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ°Ğ»ĞµÑ€Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· API
curl -u admin:admin http://localhost:3000/api/alertmanager/grafana/api/v1/alerts

# ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° port-forward
pkill -f "kubectl port-forward.*grafana"
```

### **8.3 Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ»ĞµÑ€Ñ‚Ğ¾Ğ²**

#### **Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ»ĞµÑ€Ñ‚Ğ°:**
```bash
# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ° Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸ĞµĞ¼ CPU
cat > test-high-cpu.yaml << 'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: cpu-stress-test
  namespace: default
spec:
  containers:
  - name: cpu-stress
    image: progrium/stress
    args: ["--cpu", "2", "--timeout", "300s"]
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 2000m
        memory: 256Mi
EOF

kubectl apply -f test-high-cpu.yaml

# ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ»ĞµÑ€Ñ‚Ğ° (5-10 Ğ¼Ğ¸Ğ½ÑƒÑ‚)
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ² Grafana Ğ¸ AlertManager

# Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ°
kubectl delete -f test-high-cpu.yaml
rm test-high-cpu.yaml
```

---

## **Ğ¨Ğ°Ğ³ 9: Backup Ğ¸ Disaster Recovery**

### **9.1 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ backup ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°**
```bash
cat > ha/scripts/backup-monitoring.sh << 'EOF'
#!/bin/bash

# Monitoring Backup Script
set -e

BACKUP_DIR="/tmp/monitoring-backup-$(date +%Y%m%d-%H%M%S)"
mkdir -p $BACKUP_DIR

echo "ğŸ”„ Creating monitoring backup..."

# Backup Prometheus data
echo "ğŸ“Š Backing up Prometheus data..."
kubectl exec -n monitoring deployment/prometheus-server -- tar czf - /prometheus | \
  cat > $BACKUP_DIR/prometheus-data.tar.gz

# Backup Grafana data
echo "ğŸ“ˆ Backing up Grafana data..."
kubectl exec -n monitoring deployment/grafana -- tar czf - /var/lib/grafana | \
  cat > $BACKUP_DIR/grafana-data.tar.gz

# Backup configurations
echo "âš™ï¸ Backing up configurations..."
kubectl get configmaps -n monitoring -o yaml > $BACKUP_DIR/configmaps.yaml
kubectl get secrets -n monitoring -o yaml > $BACKUP_DIR/secrets.yaml
kubectl get pvc -n monitoring -o yaml > $BACKUP_DIR/pvc.yaml

# Create archive
echo "ğŸ“¦ Creating final archive..."
tar czf monitoring-backup-$(date +%Y%m%d-%H%M%S).tar.gz -C /tmp $(basename $BACKUP_DIR)

echo "âœ… Backup completed: monitoring-backup-$(date +%Y%m%d-%H%M%S).tar.gz"
echo "ğŸ“ Temporary files in: $BACKUP_DIR"
EOF

chmod +x ha/scripts/backup-monitoring.sh
```

### **9.2 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ restore ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°**
```bash
mkdir -p ha/scripts

cat > ha/scripts/restore-monitoring.sh << 'EOF'
#!/bin/bash

# Monitoring Restore Script
set -e

if [ -z "$1" ]; then
    echo "Usage: $0 <backup-file.tar.gz>"
    exit 1
fi

BACKUP_FILE=$1
RESTORE_DIR="/tmp/monitoring-restore-$(date +%Y%m%d-%H%M%S)"

echo "ğŸ”„ Restoring monitoring from $BACKUP_FILE..."

# Extract backup
mkdir -p $RESTORE_DIR
tar xzf $BACKUP_FILE -C $RESTORE_DIR

# Restore configurations
echo "âš™ï¸ Restoring configurations..."
kubectl apply -f $RESTORE_DIR/*/configmaps.yaml
kubectl apply -f $RESTORE_DIR/*/secrets.yaml

# Restore PVCs (if needed)
echo "ğŸ’¾ Restoring PVCs..."
kubectl apply -f $RESTORE_DIR/*/pvc.yaml

# Wait for pods to be ready
echo "â³ Waiting for pods to be ready..."
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=prometheus -n monitoring --timeout=300s
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=grafana -n monitoring --timeout=300s

# Restore Prometheus data
echo "ğŸ“Š Restoring Prometheus data..."
kubectl exec -n monitoring deployment/prometheus-server -- rm -rf /prometheus/*
cat $RESTORE_DIR/*/prometheus-data.tar.gz | \
  kubectl exec -i -n monitoring deployment/prometheus-server -- tar xzf - -C /

# Restore Grafana data
echo "ğŸ“ˆ Restoring Grafana data..."
kubectl exec -n monitoring deployment/grafana -- rm -rf /var/lib/grafana/*
cat $RESTORE_DIR/*/grafana-data.tar.gz | \
  kubectl exec -i -n monitoring deployment/grafana -- tar xzf - -C /

# Restart services
echo "ğŸ”„ Restarting services..."
kubectl rollout restart deployment/prometheus-server -n monitoring
kubectl rollout restart deployment/grafana -n monitoring

echo "âœ… Restore completed successfully!"
echo "ğŸ§¹ Cleaning up temporary files..."
rm -rf $RESTORE_DIR
EOF

chmod +x ha/scripts/restore-monitoring.sh
```

---

## **Ğ¨Ğ°Ğ³ 10: Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Runbooks**

### **10.1 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… runbooks**
```bash
cat > ha/docs/MONITORING_RUNBOOKS.md << 'EOF'
# Monitoring Runbooks

## ğŸš¨ **Critical Alerts Response**

### **Node Down Alert**
**Symptoms:** Node becomes unreachable
**Impact:** Reduced cluster capacity, potential service disruption

**Response Steps:**
1. Check node status: `kubectl get nodes`
2. Check node events: `kubectl describe node <node-name>`
3. SSH to node (if possible): `ssh <node-ip>`
4. Check system logs: `journalctl -u kubelet`
5. Restart kubelet if needed: `systemctl restart kubelet`
6. If node is unrecoverable, drain and replace:
   ```bash
   kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data
   kubectl delete node <node-name>
   ```

### **High Memory Usage Alert**
**Symptoms:** Memory usage > 90%
**Impact:** Risk of OOM kills, performance degradation

**Response Steps:**
1. Identify memory consumers: `kubectl top pods --all-namespaces --sort-by=memory`
2. Check for memory leaks in applications
3. Scale down non-critical workloads if needed
4. Consider adding more nodes or increasing node size

### **ArgoCD Application Not Synced**
**Symptoms:** Application stuck in "OutOfSync" state
**Impact:** Deployments not applied, configuration drift

**Response Steps:**
1. Check application status: `kubectl get application <app-name> -n argocd`
2. Check sync errors: `kubectl describe application <app-name> -n argocd`
3. Manual sync: `argocd app sync <app-name>`
4. Check Git repository accessibility
5. Verify RBAC permissions

## ğŸ“Š **Monitoring Health Checks**

### **Daily Checks:**
- [ ] All monitoring pods running
- [ ] Prometheus targets healthy
- [ ] Grafana dashboards loading
- [ ] AlertManager receiving alerts
- [ ] Disk space < 80%

### **Weekly Checks:**
- [ ] Review alert history
- [ ] Update dashboards if needed
- [ ] Check backup integrity
- [ ] Review resource usage trends

### **Monthly Checks:**
- [ ] Update monitoring stack
- [ ] Review and tune alert thresholds
- [ ] Capacity planning review
- [ ] Security updates
EOF
```

---

## **ğŸ¯ Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ÑĞµĞ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ**

### **ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°:**

```bash
#!/bin/bash
# Comprehensive Monitoring Test Script

echo "ğŸ§ª Starting comprehensive monitoring test..."

# Test 1: Component Health
echo "1ï¸âƒ£ Testing component health..."
kubectl get pods -n monitoring
if [ $? -eq 0 ]; then
    echo "âœ… All monitoring pods accessible"
else
    echo "âŒ Failed to access monitoring pods"
    exit 1
fi

# Test 2: Prometheus Metrics
echo "2ï¸âƒ£ Testing Prometheus metrics..."
kubectl port-forward -n monitoring svc/prometheus-server 9090:80 &
PF_PID=$!
sleep 5

METRICS_COUNT=$(curl -s http://localhost:9090/api/v1/label/__name__/values | jq '.data | length')
if [ "$METRICS_COUNT" -gt 100 ]; then
    echo "âœ… Prometheus collecting metrics ($METRICS_COUNT metrics found)"
else
    echo "âŒ Prometheus not collecting enough metrics"
fi

kill $PF_PID

# Test 3: Grafana Dashboards
echo "3ï¸âƒ£ Testing Grafana dashboards..."
kubectl port-forward -n monitoring svc/grafana 3000:80 &
PF_PID=$!
sleep 5

GRAFANA_HEALTH=$(curl -s http://localhost:3000/api/health | jq -r '.database')
if [ "$GRAFANA_HEALTH" = "ok" ]; then
    echo "âœ… Grafana healthy and accessible"
else
    echo "âŒ Grafana health check failed"
fi

kill $PF_PID

# Test 4: AlertManager
echo "4ï¸âƒ£ Testing AlertManager..."
kubectl port-forward -n monitoring svc/alertmanager 9093:9093 &
PF_PID=$!
sleep 5

AM_STATUS=$(curl -s http://localhost:9093/api/v1/status | jq -r '.status')
if [ "$AM_STATUS" = "success" ]; then
    echo "âœ… AlertManager operational"
else
    echo "âŒ AlertManager not responding"
fi

kill $PF_PID

# Test 5: Ingress Connectivity
echo "5ï¸âƒ£ Testing Ingress connectivity..."
INGRESS_IP=$(kubectl get svc -n ingress-nginx nginx-ingress-ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

if curl -k -H "Host: grafana.hashfoundry.local" https://$INGRESS_IP/ | grep -q "Grafana"; then
    echo "âœ… Grafana accessible via Ingress"
else
    echo "âŒ Grafana not accessible via Ingress"
fi

# Test 6: Storage Persistence
echo "6ï¸âƒ£ Testing storage persistence..."
PVC_COUNT=$(kubectl get pvc -n monitoring --no-headers | wc -l)
if [ "$PVC_COUNT" -ge 3 ]; then
    echo "âœ… Persistent storage configured ($PVC_COUNT PVCs found)"
else
    echo "âŒ Insufficient persistent storage"
fi

# Test 7: High Availability
echo "7ï¸âƒ£ Testing high availability..."
PROMETHEUS_REPLICAS=$(kubectl get deployment prometheus-server -n monitoring -o jsonpath='{.status.readyReplicas}')
GRAFANA_REPLICAS=$(kubectl get deployment grafana -n monitoring -o jsonpath='{.status.readyReplicas}')

if [ "$PROMETHEUS_REPLICAS" -ge 1 ] && [ "$GRAFANA_REPLICAS" -ge 1 ]; then
    echo "âœ… HA configuration verified"
else
    echo "âŒ HA configuration issues detected"
fi

echo ""
echo "ğŸ‰ Comprehensive monitoring test completed!"
echo ""
echo "ğŸ“Š Access URLs (add to /etc/hosts):"
echo "$INGRESS_IP grafana.hashfoundry.local"
echo "$INGRESS_IP prometheus.hashfoundry.local"
echo "$INGRESS_IP alerts.hashfoundry.local"
echo ""
echo "ğŸ” Default credentials:"
echo "Grafana: admin / admin"
echo ""
echo "ğŸ“š Next steps:"
echo "1. Configure email/Slack notifications in AlertManager"
echo "2. Customize Grafana dashboards for your needs"
echo "3. Set up regular backups"
echo "4. Review and tune alert thresholds"
```

---

## **ğŸ“‹ Ğ—Ğ°ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ**

### **Ğ§Ñ‚Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¾:**

âœ… **Production-ready Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³** Ñ Prometheus + Grafana Stack  
âœ… **High Availability** ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²  
âœ… **Infrastructure as Code** Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Helm Charts  
âœ… **ArgoCD Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ** Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ  
âœ… **Comprehensive alerting** Ñ AlertManager  
âœ… **Storage monitoring** Ñ NFS Exporter  
âœ… **Backup/Restore** Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹  
âœ… **Operational runbooks** Ğ´Ğ»Ñ troubleshooting  
âœ… **ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ** Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²  

### **ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼ IaC:**

âœ… **Declarative** - Ğ²ÑĞµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ  
âœ… **Version controlled** - Ğ²ÑĞµ Ğ² Git Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸  
âœ… **Automated deployment** - Ñ‡ĞµÑ€ĞµĞ· ArgoCD  
âœ… **Reproducible** - Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ Ğ² Ğ»ÑĞ±Ğ¾Ğ¼ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğµ  
âœ… **Scalable** - Ğ»ĞµĞ³ĞºĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ  

### **Production Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ:**

âœ… **Monitoring coverage** - Ğ²ÑĞµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹  
âœ… **Alerting strategy** - ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ, Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ  
âœ… **High availability** - Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²  
âœ… **Data persistence** - ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹  
âœ… **Security** - RBAC, network policies, secure access  
âœ… **Operational procedures** - backup, restore, troubleshooting  

**Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ° Ğº production Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ!**

---

**Ğ”Ğ°Ñ‚Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ**: 16.07.2025  
**Ğ’ĞµÑ€ÑĞ¸Ñ**: 1.0.0  
**Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ**: âœ… Production Ready  
**Ğ¡Ğ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ IaC**: âœ… Full Compliance
