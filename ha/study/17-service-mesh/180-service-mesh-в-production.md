# 180. Service mesh Ğ² production

## ğŸ¯ **Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ service mesh Ğ² production?**

**Service mesh Ğ² production** - ÑÑ‚Ğ¾ enterprise-grade Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, comprehensive Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼, security hardening Ğ¸ disaster recovery Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ñ, canary deployments, SLI/SLO Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ runbooks Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.

## ğŸ—ï¸ **ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ production:**

### **1. High Availability**
- Multi-zone control plane deployment
- Automatic failover mechanisms
- Circuit breakers Ğ¸ retry policies
- Load balancing strategies

### **2. Performance & Scalability**
- P99 latency < 100ms overhead
- Support 10k+ RPS per service
- Horizontal auto-scaling
- Resource optimization

### **3. Security & Compliance**
- mTLS everywhere
- Zero-trust policies
- Audit logging
- Compliance standards (SOC 2, GDPR)

### **4. Observability**
- Golden signals monitoring
- SLI/SLO tracking
- End-to-end tracing
- Comprehensive alerting

## ğŸ“Š **ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ğ²Ğ°ÑˆĞµĞ³Ğ¾ HA ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°:**

### **1. Production-ready Istio deployment:**
```bash
#!/bin/bash
# deploy-production-istio.sh

echo "ğŸš€ Production Istio Deployment"

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ production IstioOperator
kubectl apply -f - << EOF
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: production
  namespace: istio-system
spec:
  values:
    global:
      meshID: hashfoundry-production-mesh
      cluster: hashfoundry-production
      network: production-network
      proxy:
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 2000m
            memory: 1Gi
    pilot:
      env:
        PILOT_ENABLE_WORKLOAD_ENTRY_AUTOREGISTRATION: true
        PILOT_ENABLE_CROSS_CLUSTER_WORKLOAD_ENTRY: true
        PILOT_TRACE_SAMPLING: 1.0
        PILOT_ENABLE_STATUS: true
    telemetryV2:
      enabled: true
      prometheus:
        configOverride:
          metric_relabeling_configs:
          - source_labels: [__name__]
            regex: 'istio_.*'
            target_label: __tmp_istio_metric
          - source_labels: [__tmp_istio_metric]
            regex: '.*'
            target_label: __name__
            replacement: '${1}'
  components:
    pilot:
      k8s:
        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        hpaSpec:
          minReplicas: 3
          maxReplicas: 10
          metrics:
          - type: Resource
            resource:
              name: cpu
              target:
                type: Utilization
                averageUtilization: 70
          - type: Resource
            resource:
              name: memory
              target:
                type: Utilization
                averageUtilization: 80
        podDisruptionBudget:
          minAvailable: 2
        env:
          - name: PILOT_ENABLE_STATUS
            value: "true"
          - name: PILOT_ENABLE_WORKLOAD_ENTRY_AUTOREGISTRATION
            value: "true"
        nodeSelector:
          node-role: control-plane
        tolerations:
        - key: node-role
          operator: Equal
          value: control-plane
          effect: NoSchedule
    ingressGateways:
    - name: istio-ingressgateway
      enabled: true
      k8s:
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 2000m
            memory: 1Gi
        hpaSpec:
          minReplicas: 3
          maxReplicas: 10
          metrics:
          - type: Resource
            resource:
              name: cpu
              target:
                type: Utilization
                averageUtilization: 70
        podDisruptionBudget:
          minAvailable: 2
        service:
          type: LoadBalancer
          loadBalancerSourceRanges:
          - "10.0.0.0/8"
          - "172.16.0.0/12"
          - "192.168.0.0/16"
        nodeSelector:
          node-role: worker
    egressGateways:
    - name: istio-egressgateway
      enabled: true
      k8s:
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        hpaSpec:
          minReplicas: 2
          maxReplicas: 5
        podDisruptionBudget:
          minAvailable: 1
        nodeSelector:
          node-role: worker
EOF

# ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸
kubectl wait --for=condition=available deployment/istiod -n istio-system --timeout=600s

echo "âœ… Production Istio Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚"
```

### **2. ĞŸĞ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° service mesh:**
```bash
#!/bin/bash
# production-migration.sh

echo "ğŸš€ Production Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Service Mesh"

# ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ
MIGRATION_PHASE=${1:-"phase1"}
NAMESPACE=${2:-"production"}

# Phase 1: ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹
phase1_infrastructure() {
    echo "ğŸ“‹ Phase 1: ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹"
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ production namespace Ñ labels
    kubectl create namespace $NAMESPACE --dry-run=client -o yaml | \
    kubectl apply -f -
    
    kubectl label namespace $NAMESPACE \
        environment=production \
        istio-injection=enabled \
        --overwrite
    
    # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° network policies
    kubectl apply -f - << EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: istio-mesh-policy
  namespace: $NAMESPACE
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: istio-system
    - namespaceSelector:
        matchLabels:
          environment: production
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: istio-system
    - namespaceSelector:
        matchLabels:
          environment: production
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 443
EOF
    
    # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° default security policies
    kubectl apply -f - << EOF
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: $NAMESPACE
spec:
  mtls:
    mode: STRICT
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: default
  namespace: $NAMESPACE
spec:
  host: "*.local"
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
EOF
    
    echo "âœ… Phase 1 Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°"
}

# Phase 2: Pilot services (Ğ½ĞµĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ)
phase2_pilot_services() {
    echo "ğŸ§ª Phase 2: Pilot services"
    
    local pilot_services=("logging-service" "metrics-collector" "health-checker")
    
    for service in "${pilot_services[@]}"; do
        echo "ĞœĞ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ñ pilot ÑĞµÑ€Ğ²Ğ¸ÑĞ°: $service"
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ deployment
        if ! kubectl get deployment $service -n $NAMESPACE >/dev/null 2>&1; then
            echo "âš ï¸ Deployment $service Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼"
            continue
        fi
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ backup ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
        kubectl get deployment $service -n $NAMESPACE -o yaml > /tmp/${service}-backup-$(date +%s).yaml
        
        # Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ sidecar injection
        kubectl patch deployment $service -n $NAMESPACE -p '{"spec":{"template":{"metadata":{"annotations":{"sidecar.istio.io/inject":"true"}}}}}'
        
        # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ rollout
        kubectl rollout status deployment/$service -n $NAMESPACE --timeout=300s
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ
        local ready_replicas=$(kubectl get deployment $service -n $NAMESPACE -o jsonpath='{.status.readyReplicas}')
        local desired_replicas=$(kubectl get deployment $service -n $NAMESPACE -o jsonpath='{.spec.replicas}')
        
        if [ "$ready_replicas" = "$desired_replicas" ]; then
            echo "âœ… $service Ğ¼Ğ¸Ğ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾"
        else
            echo "âŒ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ $service"
            kubectl describe deployment $service -n $NAMESPACE
        fi
    done
    
    echo "âœ… Phase 2 Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°"
}

# Phase 3: Core services (Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ)
phase3_core_services() {
    echo "ğŸ—ï¸ Phase 3: Core services"
    
    local core_services=("user-service" "notification-service" "analytics-service")
    
    for service in "${core_services[@]}"; do
        echo "ĞœĞ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ñ core ÑĞµÑ€Ğ²Ğ¸ÑĞ°: $service"
        
        if ! kubectl get deployment $service -n $NAMESPACE >/dev/null 2>&1; then
            echo "âš ï¸ Deployment $service Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼"
            continue
        fi
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ backup
        kubectl get deployment $service -n $NAMESPACE -o yaml > /tmp/${service}-backup-$(date +%s).yaml
        
        # Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ replicas Ğ´Ğ»Ñ canary
        local current_replicas=$(kubectl get deployment $service -n $NAMESPACE -o jsonpath='{.spec.replicas}')
        local canary_replicas=$((current_replicas * 2))
        
        kubectl scale deployment $service -n $NAMESPACE --replicas=$canary_replicas
        kubectl rollout status deployment/$service -n $NAMESPACE
        
        # Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ sidecar Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… pods
        kubectl patch deployment $service -n $NAMESPACE -p '{"spec":{"template":{"metadata":{"annotations":{"sidecar.istio.io/inject":"true"}}}}}'
        
        # ĞŸĞ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ
        kubectl patch deployment $service -n $NAMESPACE -p '{"spec":{"strategy":{"rollingUpdate":{"maxSurge":"25%","maxUnavailable":"25%"}}}}'
        kubectl rollout status deployment/$service -n $NAMESPACE --timeout=600s
        
        # ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ 5 Ğ¼Ğ¸Ğ½ÑƒÑ‚
        echo "ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ $service..."
        for i in {1..5}; do
            sleep 60
            echo "ĞœĞ¸Ğ½ÑƒÑ‚Ğ° $i: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº..."
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° error rate Ñ‡ĞµÑ€ĞµĞ· kubectl (ĞµÑĞ»Ğ¸ Prometheus Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½)
            local pod_count=$(kubectl get pods -n $NAMESPACE -l app=$service --field-selector=status.phase=Running | wc -l)
            echo "Running pods: $((pod_count - 1))"  # -1 Ğ´Ğ»Ñ header
        done
        
        # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚ Ğº Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ replicas
        kubectl scale deployment $service -n $NAMESPACE --replicas=$current_replicas
        
        echo "âœ… $service Ğ¼Ğ¸Ğ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾"
    done
    
    echo "âœ… Phase 3 Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°"
}

# Phase 4: Critical services (ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ)
phase4_critical_services() {
    echo "ğŸ”¥ Phase 4: Critical services"
    
    local critical_services=("payment-service" "auth-service" "order-service")
    
    for service in "${critical_services[@]}"; do
        echo "ĞœĞ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑĞ°: $service"
        
        if ! kubectl get deployment $service -n $NAMESPACE >/dev/null 2>&1; then
            echo "âš ï¸ Deployment $service Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½, ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ demo deployment"
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ demo deployment Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸
            kubectl apply -f - << EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: $service
  namespace: $NAMESPACE
  labels:
    app: $service
    version: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: $service
      version: blue
  template:
    metadata:
      labels:
        app: $service
        version: blue
    spec:
      containers:
      - name: $service
        image: nginx:1.21
        ports:
        - containerPort: 80
        env:
        - name: SERVICE_NAME
          value: "$service"
        - name: VERSION
          value: "blue"
---
apiVersion: v1
kind: Service
metadata:
  name: $service
  namespace: $NAMESPACE
spec:
  selector:
    app: $service
  ports:
  - port: 80
    targetPort: 80
EOF
            
            kubectl rollout status deployment/$service -n $NAMESPACE
        fi
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ backup
        kubectl get deployment $service -n $NAMESPACE -o yaml > /tmp/${service}-backup-$(date +%s).yaml
        
        # Blue-green deployment
        kubectl label deployment $service -n $NAMESPACE version=blue --overwrite
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ green version Ñ sidecar
        kubectl get deployment $service -n $NAMESPACE -o yaml | \
            sed 's/version: blue/version: green/g' | \
            sed 's/name: '$service'/name: '$service'-green/g' | \
            kubectl apply -f -
        
        kubectl patch deployment ${service}-green -n $NAMESPACE -p '{"spec":{"template":{"metadata":{"annotations":{"sidecar.istio.io/inject":"true"},"labels":{"version":"green"}}}}}'
        
        # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ green
        kubectl rollout status deployment/${service}-green -n $NAMESPACE --timeout=600s
        
        # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° traffic splitting
        kubectl apply -f - << EOF
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: ${service}-traffic-split
  namespace: $NAMESPACE
spec:
  hosts:
  - $service
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: $service
        subset: green
  - route:
    - destination:
        host: $service
        subset: blue
      weight: 90
    - destination:
        host: $service
        subset: green
      weight: 10
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: ${service}-destination
  namespace: $NAMESPACE
spec:
  host: $service
  subsets:
  - name: blue
    labels:
      version: blue
  - name: green
    labels:
      version: green
EOF
        
        # ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ 10 Ğ¼Ğ¸Ğ½ÑƒÑ‚
        echo "ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ $service Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ 10 Ğ¼Ğ¸Ğ½ÑƒÑ‚..."
        for i in {1..10}; do
            sleep 60
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° pods
            local green_pods=$(kubectl get pods -n $NAMESPACE -l app=$service,version=green --field-selector=status.phase=Running | wc -l)
            local blue_pods=$(kubectl get pods -n $NAMESPACE -l app=$service,version=blue --field-selector=status.phase=Running | wc -l)
            
            echo "ĞœĞ¸Ğ½ÑƒÑ‚Ğ° $i: Green pods: $((green_pods - 1)), Blue pods: $((blue_pods - 1))"
        done
        
        # ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 100% green
        kubectl patch virtualservice ${service}-traffic-split -n $NAMESPACE --type='merge' -p='{"spec":{"http":[{"route":[{"destination":{"host":"'$service'","subset":"green"},"weight":100}]}]}}'
        
        # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ 2 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
        sleep 120
        
        # Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ blue version
        kubectl delete deployment $service -n $NAMESPACE
        
        # ĞŸĞµÑ€ĞµĞ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ green Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹
        kubectl patch deployment ${service}-green -n $NAMESPACE -p='{"metadata":{"name":"'$service'"}}'
        kubectl patch deployment ${service}-green -n $NAMESPACE -p='{"spec":{"template":{"metadata":{"labels":{"version":"v1"}}}}}'
        
        # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° traffic splitting
        kubectl delete virtualservice ${service}-traffic-split -n $NAMESPACE
        kubectl delete destinationrule ${service}-destination -n $NAMESPACE
        
        echo "âœ… $service Ğ¼Ğ¸Ğ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° service mesh"
    done
    
    echo "âœ… Phase 4 Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°"
}

# Phase 5: Ğ¤Ğ¸Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³
phase5_finalization() {
    echo "ğŸ¯ Phase 5: Ğ¤Ğ¸Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ"
    
    # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° production telemetry
    kubectl apply -f - << EOF
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: production-telemetry
  namespace: $NAMESPACE
spec:
  metrics:
  - providers:
    - name: prometheus
  - overrides:
    - match:
        metric: ALL_METRICS
      tagOverrides:
        request_protocol:
          operation: UPSERT
          value: "%{REQUEST_PROTOCOL}"
        response_flags:
          operation: UPSERT
          value: "%{RESPONSE_FLAGS}"
  tracing:
  - providers:
    - name: jaeger
  - customTags:
      user_id:
        header:
          name: "user-id"
      request_id:
        header:
          name: "x-request-id"
      environment:
        literal:
          value: "production"
  accessLogging:
  - providers:
    - name: otel
  - format:
      text: |
        [%START_TIME%] "%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%"
        %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT%
        %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% "%REQ(X-FORWARDED-FOR)%"
        "%REQ(USER-AGENT)%" "%REQ(X-REQUEST-ID)%" "%REQ(:AUTHORITY)%" "%UPSTREAM_HOST%"
        environment="production"
EOF
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°
    echo "=== Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ==="
    kubectl get pods -n $NAMESPACE
    kubectl get virtualservices -n $NAMESPACE
    kubectl get destinationrules -n $NAMESPACE
    kubectl get peerauthentications -n $NAMESPACE
    
    echo "âœ… Phase 5 Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°"
    echo "ğŸ‰ Production Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾!"
}

# ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ°
case "$MIGRATION_PHASE" in
    phase1)
        phase1_infrastructure
        ;;
    phase2)
        phase2_pilot_services
        ;;
    phase3)
        phase3_core_services
        ;;
    phase4)
        phase4_critical_services
        ;;
    phase5)
        phase5_finalization
        ;;
    all)
        phase1_infrastructure
        phase2_pilot_services
        phase3_core_services
        phase4_critical_services
        phase5_finalization
        ;;
    *)
        echo "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: $0 {phase1|phase2|phase3|phase4|phase5|all} [namespace]"
        echo ""
        echo "Phases:"
        echo "  phase1 - ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹"
        echo "  phase2 - Pilot services (Ğ½ĞµĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ)"
        echo "  phase3 - Core services (Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ)"
        echo "  phase4 - Critical services (ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ)"
        echo "  phase5 - Ğ¤Ğ¸Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ"
        echo "  all    - Ğ’ÑĞµ Ñ„Ğ°Ğ·Ñ‹"
        exit 1
        ;;
esac
```

### **3. Production SLI/SLO Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³:**
```yaml
# production-slos.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: production-slos
  namespace: istio-system
spec:
  groups:
  - name: sli-slo.rules
    interval: 30s
    rules:
    # Availability SLI
    - record: sli:availability:rate5m
      expr: |
        sum(rate(istio_requests_total{response_code!~"5.*"}[5m])) by (destination_service_name, destination_namespace)
        /
        sum(rate(istio_requests_total[5m])) by (destination_service_name, destination_namespace)
    
    # Latency SLI
    - record: sli:latency:p99:5m
      expr: |
        histogram_quantile(0.99,
          sum(rate(istio_request_duration_milliseconds_bucket[5m])) by (destination_service_name, destination_namespace, le)
        )
    
    # Latency SLI P95
    - record: sli:latency:p95:5m
      expr: |
        histogram_quantile(0.95,
          sum(rate(istio_request_duration_milliseconds_bucket[5m])) by (destination_service_name, destination_namespace, le)
        )
    
    # Throughput SLI
    - record: sli:throughput:rate5m
      expr: |
        sum(rate(istio_requests_total[5m])) by (destination_service_name, destination_namespace)
    
    # Error Budget Alerts
    - alert: SLOAvailabilityBreach
      expr: |
        (
          sli:availability:rate5m < 0.999
        ) and (
          sli:availability:rate5m > 0
        )
      for: 2m
      labels:
        severity: critical
        slo: availability
      annotations:
        summary: "Service availability SLO breach"
        description: "Service {{ $labels.destination_service_name }} availability is {{ $value | humanizePercentage }}, below 99.9% SLO"
    
    - alert: SLOLatencyBreach
      expr: |
        sli:latency:p99:5m > 100
      for: 5m
      labels:
        severity: warning
        slo: latency
      annotations:
        summary: "Service latency SLO breach"
        description: "Service {{ $labels.destination_service_name }} P99 latency is {{ $value }}ms, above 100ms SLO"
    
    # Error Budget Burn Rate
    - alert: ErrorBudgetBurnRateHigh
      expr: |
        (
          1 - sli:availability:rate5m
        ) > (
          (1 - 0.999) * 14.4  # 14.4x burn rate for 2% budget in 1 hour
        )
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "High error budget burn rate"
        description: "Service {{ $labels.destination_service_name }} is burning error budget at {{ $value | humanizePercentage }} rate"
    
    - alert: ErrorBudgetBurnRateMedium
      expr: |
        (
          1 - sli:availability:rate5m
        ) > (
          (1 - 0.999) * 6  # 6x burn rate for 5% budget in 1 hour
        )
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Medium error budget burn rate"
        description: "Service {{ $labels.destination_service_name }} is burning error budget at {{ $value | humanizePercentage }} rate"
```

### **4. Production security hardening:**
```yaml
# production-security.yaml

# Strict mTLS Ğ´Ğ»Ñ Ğ²ÑĞµĞ³Ğ¾ mesh
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: istio-system
spec:
  mtls:
    mode: STRICT
---
# Default deny-all authorization policy
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: deny-all
  namespace: production
spec:
  {}  # Deny all by default
---
# Allow internal service communication
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-internal-services
  namespace: production
spec:
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/production/sa/*"]
  - to:
    - operation:
        methods: ["GET", "POST", "PUT", "DELETE"]
  - when:
    - key: source.namespace
      values: ["production"]
---
# Allow ingress gateway
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-ingress-gateway
  namespace: production
spec:
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account"]
---
# Rate limiting
apiVersion: networking.istio.io/v1beta1
kind: EnvoyFilter
metadata:
  name: rate-limit-filter
  namespace: production
spec:
  configPatches:
  - applyTo: HTTP_FILTER
    match:
      context: SIDECAR_INBOUND
      listener:
        filterChain:
          filter:
            name: "envoy.filters.network.http_connection_manager"
    patch:
      operation: INSERT_BEFORE
      value:
        name: envoy.filters.http.local_ratelimit
        typed_config:
          "@type": type.googleapis.com/udpa.type.v1.TypedStruct
          type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit
          value:
            stat_prefix: rate_limiter
            token_bucket:
              max_tokens: 1000
              tokens_per_fill: 1000
              fill_interval: 60s
            filter_enabled:
              runtime_key: rate_limit_enabled
              default_value:
                numerator: 100
                denominator: HUNDRED
            filter_enforced:
              runtime_key: rate_limit_enforced
              default_value:
                numerator: 100
                denominator: HUNDRED
---
# Security scanning policy
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: security-scanning
  namespace: production
spec:
  selector:
    matchLabels:
      security-scan: "enabled"
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/security/sa/scanner"]
  - to:
    - operation:
        methods: ["GET"]
        paths: ["/health", "/metrics", "/ready"]
```

## ğŸš¨ **Disaster Recovery Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹:**

### **1. Backup Ğ¸ restore ÑĞºÑ€Ğ¸Ğ¿Ñ‚:**
```bash
#!/bin/bash
# disaster-recovery.sh

echo "ğŸš¨ Service Mesh Disaster Recovery"

# Backup control plane
backup_control_plane() {
    echo "ğŸ’¾ Backup control plane ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸"
    
    local backup_dir="/backup/istio-$(date +%Y%m%d-%H%M%S)"
    mkdir -p $backup_dir
    
    # Backup Istio configuration
    kubectl get istiooperator -n istio-system -o yaml > $backup_dir/istio-operator.yaml
    kubectl get configmap istio -n istio-system -o yaml > $backup_dir/istio-config.yaml
    
    # Backup certificates
    kubectl get secret cacerts -n istio-system -o yaml > $backup_dir/cacerts.yaml 2>/dev/null || echo "No custom cacerts found"
    
    # Backup custom resources
    kubectl get virtualservices --all-namespaces -o yaml > $backup_dir/virtualservices.yaml
    kubectl get destinationrules --all-namespaces -o yaml > $backup_dir/destinationrules.yaml
    kubectl get gateways --all-namespaces -o yaml > $backup_dir/gateways.yaml
    kubectl get peerauthentications --all-namespaces -o yaml > $backup_dir/peerauthentications.yaml
    kubectl get authorizationpolicies --all-namespaces -o yaml > $backup_dir/authorizationpolicies.yaml
    kubectl get serviceentries --all-namespaces -o yaml > $backup_dir/serviceentries.yaml
    kubectl get envoyfilters --all-namespaces -o yaml > $backup_dir/envoyfilters.yaml
    
    # Backup telemetry configuration
    kubectl get telemetry --all-namespaces -o yaml > $backup_dir/telemetry.yaml 2>/dev/null || echo "No telemetry resources found"
    
    # Create backup manifest
    cat > $backup_dir/backup-manifest.txt << EOF
Istio Backup Created: $(date)
Cluster: $(kubectl config current-context)
Istio Version: $(istioctl version --short 2>/dev/null || echo "Unknown")

Files included:
- istio-operator.yaml
- istio-config.yaml
- cacerts.yaml
- virtualservices.yaml
- destinationrules.yaml
- gateways.yaml
- peerauthentications.yaml
- authorizationpolicies.yaml
- serviceentries.yaml
- envoyfilters.yaml
- telemetry.yaml
EOF
    
    echo "âœ… Backup ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½ Ğ² $backup_dir"
    echo "ğŸ“‹ Manifest: $backup_dir/backup-manifest.txt"
}

# Restore control plane
restore_control_plane() {
    local backup_dir=$1
    
    if [ -z "$backup_dir" ] || [ ! -d "$backup_dir" ]; then
        echo "âŒ Backup directory Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: $backup_dir"
        exit 1
    fi
    
    echo "ğŸ”„ Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ control plane Ğ¸Ğ· $backup_dir"
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° backup manifest
    if [ -f "$backup_dir/backup-manifest.txt" ]; then
        echo "ğŸ“‹ Backup manifest:"
        cat $backup_dir/backup-manifest.txt
        echo ""
    fi
    
    # Restore certificates first (if exists)
    if [ -f "$backup_dir/cacerts.yaml" ]; then
        echo "Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ certificates..."
        kubectl apply -f $backup_dir/cacerts.yaml
    fi
    
    # Restore Istio Operator
    if [ -f "$backup_dir/istio-operator.yaml" ]; then
        echo "Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ IstioOperator..."
        kubectl apply -f $backup_dir/istio-operator.yaml
        kubectl wait --for=condition=available deployment/istiod -n istio-system --timeout=600s
    fi
    
    # Restore configuration
    if [ -f "$backup_dir/istio-config.yaml" ]; then
        echo "Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Istio config..."
        kubectl apply -f $backup_dir/istio-config.yaml
    fi
    
    # Restore custom resources
    echo "Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ custom resources..."
    for resource in virtualservices destinationrules gateways peerauthentications authorizationpolicies serviceentries envoyfilters telemetry; do
        if [ -f "$backup_dir/${resource}.yaml" ]; then
            echo "Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ $resource..."
            kubectl apply -f $backup_dir/${resource}.yaml
        fi
    done
    
    echo "âœ… Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾"
}

# Emergency rollback
emergency_rollback() {
    echo "ğŸš¨ Emergency rollback"
    
    # Disable sidecar injection
    kubectl label namespace production istio-injection-
    
    # Remove Istio resources
    kubectl delete virtualservices --all -n production
    kubectl delete destinationrules --all -n production
    kubectl delete peerauthentications --all -n production
    kubectl delete authorizationpolicies --all -n production
    
    # Restart deployments to remove sidecars
    kubectl rollout restart deployment --all -n production
    
    echo "âœ… Emergency rollback Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½"
}

# ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ°
case "$1" in
    backup)
        backup_control_plane
        ;;
    restore)
        restore_control_plane $2
        ;;
    rollback)
        emergency_rollback
        ;;
    *)
        echo "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: $0 {backup|restore|rollback} [backup_dir]"
        exit 1
        ;;
esac
```

## ğŸ“ˆ **ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ production service mesh:**

### **1. Production health checks:**
```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° health Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²
kubectl get pods -n istio-system
kubectl get pods -n production

# Proxy status
istioctl proxy-status

# Configuration analysis
istioctl analyze --all-namespaces

# Performance metrics
kubectl top pods -n istio-system --containers
kubectl top pods -n production --containers
```

### **2. SLI/SLO dashboard:**
```bash
# Port forward Ğº Grafana
kubectl port-forward svc/grafana -n monitoring 3000:80

# Key SLI metrics:
# - Availability: istio_requests_total success rate
# - Latency: istio_request_duration_milliseconds P99
# - Throughput: istio_requests_total rate
# - Error rate: istio_requests_total 5xx rate
```

## ğŸ­ **Production Ğ² Ğ²Ğ°ÑˆĞµĞ¼ HA ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğµ:**

### **1. ArgoCD Ğ² production mesh:**
```bash
# ArgoCD Ñ production Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ğ¼Ğ¸
kubectl get pods -n argocd -o wide
kubectl describe pod -n argocd -l app.kubernetes.io/name=argocd-server

# Production VirtualService Ğ´Ğ»Ñ ArgoCD
kubectl get virtualservice -n argocd
kubectl describe virtualservice argocd-server -n argocd

# mTLS Ğ´Ğ»Ñ ArgoCD
istioctl authn tls-check argocd-server.argocd.svc.cluster.local
```

### **2. Monitoring stack Ğ² production:**
```bash
# Prometheus Ñ sidecars
kubectl get pods -n monitoring -l app=prometheus-server
kubectl exec -n monitoring deployment/prometheus-server -c istio-proxy -- pilot-agent request GET stats | grep istio_requests

# Grafana Ñ sidecars
kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana
kubectl exec -n monitoring deployment/grafana -c istio-proxy -- pilot-agent request GET stats | grep istio_requests
```

### **3. Production traffic policies:**
```bash
# Circuit breaker Ğ´Ğ»Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²
kubectl apply -f - << EOF
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: argocd-circuit-breaker
  namespace: argocd
spec:
  host: argocd-server
  trafficPolicy:
    outlierDetection:
      consecutiveErrors: 3
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 10
EOF

# Retry policy
kubectl apply -f - << EOF
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: argocd-retry
  namespace: argocd
spec:
  hosts:
  - argocd-server
  http:
  - route:
    - destination:
        host: argocd-server
    retries:
      attempts: 3
      perTryTimeout: 10s
      retryOn: 5xx,gateway-error,connect-failure,refused-stream
EOF
```

## ğŸš¨ **Production troubleshooting:**

### **1. Common production issues:**
```bash
# High latency
kubectl exec -n production deployment/app -c istio-proxy -- pilot-agent request GET stats | grep duration

# Certificate expiration
kubectl exec -n production deployment/app -c istio-proxy -- openssl x509 -in /var/run/secrets/workload-spiffe-credentials/cert.pem -noout -dates

# Configuration conflicts
istioctl analyze -n production

# Resource exhaustion
kubectl top pods -n istio-system --containers
kubectl describe pod -n istio-system -l app=istiod
```

### **2. Production debugging:**
```bash
# Enable debug logging
kubectl patch deployment istiod -n istio-system -p '{"spec":{"template":{"spec":{"containers":[{"name":"discovery","args":["discovery","--log_output_level=debug"]}]}}}}'

# Collect debug info
istioctl bug-report --include ns1,ns2

# Proxy debug
kubectl exec -n production deployment/app -c istio-proxy -- pilot-agent request GET config_dump > debug-config.json
```

## ğŸ¯ **Production architecture diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Production Service Mesh                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Load Balancer (DigitalOcean)                              â”‚
â”‚  â”œâ”€â”€ External Traffic                                      â”‚
â”‚  â””â”€â”€ SSL Termination                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Istio Ingress Gateway (HA)                                â”‚
â”‚  â”œâ”€â”€ 3+ replicas across zones                              â”‚
â”‚  â”œâ”€â”€ Auto-scaling (3-10 pods)                              â”‚
â”‚  â””â”€â”€ PodDisruptionBudget                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Control Plane (istio-system)                              â”‚
â”‚  â”œâ”€â”€ Istiod (HA: 3+ replicas)                              â”‚
â”‚  â”‚   â”œâ”€â”€ Service Discovery                                 â”‚
â”‚  â”‚   â”œâ”€â”€ Certificate Authority                             â”‚
â”‚  â”‚   â””â”€â”€ Configuration Management                          â”‚
â”‚  â””â”€â”€ Monitoring Integration                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Data Plane (Production Workloads)                         â”‚
â”‚  â”œâ”€â”€ ArgoCD (GitOps)                                       â”‚
â”‚  â”‚   â”œâ”€â”€ Server pods + sidecars                            â”‚
â”‚  â”‚   â”œâ”€â”€ mTLS enforcement                                  â”‚
â”‚  â”‚   â””â”€â”€ Circuit breakers                                  â”‚
â”‚  â”œâ”€â”€ Monitoring Stack                                      â”‚
â”‚  â”‚   â”œâ”€â”€ Prometheus + sidecars                             â”‚
â”‚  â”‚   â”œâ”€â”€ Grafana + sidecars                                â”‚
â”‚  â”‚   â””â”€â”€ SLI/SLO tracking                                  â”‚
â”‚  â””â”€â”€ Application Services                                  â”‚
â”‚      â”œâ”€â”€ Microservices + sidecars                          â”‚
â”‚      â”œâ”€â”€ Zero-trust policies                               â”‚
â”‚      â””â”€â”€ Observability                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Security & Compliance                                      â”‚
â”‚  â”œâ”€â”€ mTLS everywhere (STRICT mode)                         â”‚
â”‚  â”œâ”€â”€ Authorization policies                                â”‚
â”‚  â”œâ”€â”€ Rate limiting                                         â”‚
â”‚  â””â”€â”€ Audit logging                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Disaster Recovery                                          â”‚
â”‚  â”œâ”€â”€ Configuration backups                                 â”‚
â”‚  â”œâ”€â”€ Certificate backups                                   â”‚
â”‚  â”œâ”€â”€ Emergency rollback                                    â”‚
â”‚  â””â”€â”€ Multi-zone deployment                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ **Production configuration:**

### **1. Resource optimization:**
```bash
# Istiod production resources
kubectl patch deployment istiod -n istio-system -p '{"spec":{"template":{"spec":{"containers":[{"name":"discovery","resources":{"requests":{"cpu":"500m","memory":"2Gi"},"limits":{"cpu":"2000m","memory":"4Gi"}}}]}}}}'

# Sidecar resource limits
kubectl apply -f - << EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: istio-sidecar-injector
  namespace: istio-system
data:
  values: |
    global:
      proxy:
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 2000m
            memory: 1Gi
EOF
```

### **2. Performance tuning:**
```bash
# Pilot performance settings
kubectl patch deployment istiod -n istio-system -p '{"spec":{"template":{"spec":{"containers":[{"name":"discovery","env":[{"name":"PILOT_PUSH_THROTTLE","value":"100"},{"name":"PILOT_DEBOUNCE_AFTER","value":"100ms"},{"name":"PILOT_DEBOUNCE_MAX","value":"10s"}]}]}}}}'

# Envoy performance
kubectl apply -f - << EOF
apiVersion: networking.istio.io/v1beta1
kind: EnvoyFilter
metadata:
  name: performance-tuning
  namespace: istio-system
spec:
  configPatches:
  - applyTo: BOOTSTRAP
    patch:
      operation: MERGE
      value:
        stats_config:
          stats_tags:
          - tag_name: "custom_tag"
            regex: "^custom_metric_(.+)$"
EOF
```

## ğŸ¯ **Best Practices Ğ´Ğ»Ñ production:**

### **1. Deployment:**
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ñ
- ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞ¹Ñ‚Ğµ canary deployments
- ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹Ñ‚Ğµ proper health checks
- ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€ÑŒÑ‚Ğµ SLI/SLO Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸

### **2. Security:**
- Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚Ğµ STRICT mTLS
- ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞ¹Ñ‚Ğµ zero-trust policies
- ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹Ñ‚Ğµ rate limiting
- Ğ’ĞµĞ´Ğ¸Ñ‚Ğµ audit logs

### **3. Operations:**
- ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ backup Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹
- ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ÑŒÑ‚Ğµ emergency runbooks
- ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€ÑŒÑ‚Ğµ resource usage
- ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ capacity

### **4. Monitoring:**
- ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹Ñ‚Ğµ comprehensive alerting
- ĞÑ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ¹Ñ‚Ğµ golden signals
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ distributed tracing
- ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ error budgets

**Production service mesh Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸!**
