# 172. Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ service mesh

## ğŸ¯ **Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ service mesh Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹?**

**Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ service mesh Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹** â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Istio vs Linkerd vs Consul Connect vs AWS App Mesh vs Cilium Ğ¿Ğ¾ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼ performance overhead, feature completeness, operational complexity, security capabilities, observability depth Ğ¸ ecosystem integration Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.

## ğŸ—ï¸ **ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ service mesh Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ:**

### **1. Istio - Enterprise-grade Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°**
- **Strengths**: ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, rich traffic management
- **Weaknesses**: Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ resource overhead
- **Best for**: Enterprise environments, complex microservices
- **Proxy**: Envoy, Control Plane: Istiod

### **2. Linkerd - Simplicity-focused Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ**
- **Strengths**: ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ°, Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¹ overhead, Rust-based proxy
- **Weaknesses**: ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¼ĞµĞ½ÑŒÑˆĞµ features
- **Best for**: Performance-critical apps, quick adoption
- **Proxy**: Linkerd2-proxy, Control Plane: Linkerd Controller

### **3. Consul Connect - HashiCorp ecosystem**
- **Strengths**: Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Vault/Nomad, multi-platform
- **Weaknesses**: ĞœĞµĞ½ÑŒÑˆĞµ Kubernetes-native features
- **Best for**: HashiCorp environments, hybrid deployments
- **Proxy**: Envoy/Built-in, Control Plane: Consul

### **4. AWS App Mesh - Cloud-native Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ**
- **Strengths**: AWS integration, managed service
- **Weaknesses**: Vendor lock-in, AWS-specific
- **Best for**: AWS-heavy workloads, managed operations
- **Proxy**: Envoy, Control Plane: AWS Managed

### **5. Cilium Service Mesh - eBPF-powered**
- **Strengths**: eBPF performance, network-focused
- **Weaknesses**: ĞĞ¾Ğ²Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ°Ñ Ğ·Ñ€ĞµĞ»Ğ¾ÑÑ‚ÑŒ
- **Best for**: Network-intensive apps, performance optimization
- **Proxy**: eBPF, Control Plane: Cilium Agent

## ğŸ“Š **ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ğ²Ğ°ÑˆĞµĞ³Ğ¾ HA ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°:**

### **1. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… service mesh Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹:**
```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… service mesh Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹
kubectl get namespace istio-system linkerd cilium-system consul --ignore-not-found
kubectl get pods -n kube-system -l k8s-app=cilium

# ĞĞ½Ğ°Ğ»Ğ¸Ğ· CNI ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
kubectl get pods -n kube-system -o jsonpath='{.items[*].spec.containers[*].image}' | grep -E "(calico|cilium|flannel|weave)"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ service mesh
kubectl top nodes
kubectl describe nodes | grep -E "(Capacity|Allocatable)" -A 5

# ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ network policy support
kubectl get networkpolicies --all-namespaces
```

### **2. Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° service mesh readiness:**
```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Kubernetes Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
kubectl version --short

# ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²
kubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu,MEMORY:.status.capacity.memory,PODS:.status.capacity.pods

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° storage classes Ğ´Ğ»Ñ persistent volumes
kubectl get storageclass

# ĞĞ½Ğ°Ğ»Ğ¸Ğ· load balancer capabilities
kubectl get svc -o wide | grep LoadBalancer
```

### **3. ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ service mesh performance baseline:**
```bash
# Baseline network performance
kubectl run network-test --image=busybox --rm -i --restart=Never -- sh -c "time wget -q -O- http://prometheus-server.monitoring/ > /dev/null"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° current latency Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµÑ€Ğ²Ğ¸ÑĞ°Ğ¼Ğ¸
kubectl exec deployment/argocd-server -n argocd -- time curl -s http://grafana.monitoring/ > /dev/null

# ĞĞ½Ğ°Ğ»Ğ¸Ğ· current resource usage
kubectl top pods --all-namespaces --containers | head -20
```

## ğŸ”„ **Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ comprehensive service mesh comparison:**

### **1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ multi-mesh evaluation framework:**
```bash
# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ service-mesh-evaluator.sh
cat << 'EOF' > service-mesh-evaluator.sh
#!/bin/bash

echo "ğŸ” Comprehensive Service Mesh Evaluation Framework"
echo "================================================="

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ…
EVALUATION_NAMESPACE="mesh-evaluation"
TIMESTAMP=$(date +%Y%m%d-%H%M%S)
EVALUATION_LOG="/var/log/service-mesh-evaluation-$TIMESTAMP.log"

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $EVALUATION_LOG
}

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° cluster readiness
analyze_cluster_readiness() {
    log "ğŸ¥ ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ service mesh"
    
    local readiness_report="/tmp/cluster-readiness-$TIMESTAMP.json"
    
    # Comprehensive cluster assessment
    cat > $readiness_report << READINESS_REPORT_EOF
{
  "timestamp": "$(date -Iseconds)",
  "cluster": "$(kubectl config current-context)",
  "cluster_readiness": {
    "kubernetes_version": "$(kubectl version --short 2>/dev/null | grep Server | awk '{print $3}' || echo "unknown")",
    "node_resources": {
      "total_nodes": $(kubectl get nodes --no-headers | wc -l),
      "ready_nodes": $(kubectl get nodes --no-headers | grep Ready | wc -l),
      "total_cpu": "$(kubectl get nodes -o jsonpath='{.items[*].status.capacity.cpu}' | tr ' ' '\n' | awk '{sum += $1} END {print sum}')cores",
      "total_memory": "$(kubectl get nodes -o jsonpath='{.items[*].status.capacity.memory}' | tr ' ' '\n' | sed 's/Ki$//' | awk '{sum += $1} END {print sum/1024/1024 "GB"}')",
      "total_pods_capacity": $(kubectl get nodes -o jsonpath='{.items[*].status.capacity.pods}' | tr ' ' '\n' | awk '{sum += $1} END {print sum}')
    },
    "network_capabilities": {
      "cni_plugin": "$(kubectl get pods -n kube-system -o jsonpath='{.items[*].spec.containers[*].image}' | grep -o -E '(calico|cilium|flannel|weave)' | head -1 || echo "unknown")",
      "network_policies_supported": $(kubectl get networkpolicies --all-namespaces --no-headers | wc -l | awk '{if($1>0) print "true"; else print "false"}'),
      "load_balancer_available": $(kubectl get svc --all-namespaces -o jsonpath='{.items[?(@.spec.type=="LoadBalancer")].metadata.name}' | wc -w | awk '{if($1>0) print "true"; else print "false"}')
    },
    "storage_capabilities": {
      "storage_classes": $(kubectl get storageclass --no-headers | wc -l),
      "default_storage_class": "$(kubectl get storageclass -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class=="true")].metadata.name}' || echo "none")",
      "persistent_volumes": $(kubectl get pv --no-headers | wc -l)
    }
  },
  "service_mesh_compatibility": {
    "istio_compatible": $(kubectl version --short 2>/dev/null | grep Server | awk '{print $3}' | grep -E "v1\.(2[0-9]|3[0-9])" && echo "true" || echo "false"),
    "linkerd_compatible": $(kubectl version --short 2>/dev/null | grep Server | awk '{print $3}' | grep -E "v1\.(1[8-9]|2[0-9]|3[0-9])" && echo "true" || echo "false"),
    "consul_compatible": $(kubectl version --short 2>/dev/null | grep Server | awk '{print $3}' | grep -E "v1\.(1[6-9]|2[0-9]|3[0-9])" && echo "true" || echo "false"),
    "cilium_compatible": $(kubectl get pods -n kube-system -l k8s-app=cilium --no-headers | wc -l | awk '{if($1>0) print "true"; else print "false"}')
  }
}
READINESS_REPORT_EOF
    
    log "ğŸ“„ Cluster readiness report: $readiness_report"
    
    # ĞšÑ€Ğ°Ñ‚ĞºĞ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    local total_cpu=$(kubectl get nodes -o jsonpath='{.items[*].status.capacity.cpu}' | tr ' ' '\n' | awk '{sum += $1} END {print sum}')
    local ready_nodes=$(kubectl get nodes --no-headers | grep Ready | wc -l)
    local cni_plugin=$(kubectl get pods -n kube-system -o jsonpath='{.items[*].spec.containers[*].image}' | grep -o -E '(calico|cilium|flannel|weave)' | head -1 || echo "unknown")
    
    log "ğŸ¯ Cluster Readiness Summary:"
    log "  ğŸ–¥ï¸ Ready nodes: $ready_nodes"
    log "  âš¡ Total CPU: ${total_cpu} cores"
    log "  ğŸŒ CNI plugin: $cni_plugin"
    
    return 0
}

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ evaluation environment
create_evaluation_environment() {
    log "ğŸ—ï¸ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ evaluation environment"
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ namespace Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
    kubectl create namespace $EVALUATION_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -
    
    # Ğ Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ test applications
    kubectl apply -f - << TEST_APPS_EOF
# Frontend application
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: $EVALUATION_NAMESPACE
  labels:
    app: frontend
    tier: web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
        tier: web
    spec:
      containers:
      - name: frontend
        image: nginx:1.21
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 256Mi
        env:
        - name: BACKEND_URL
          value: "http://backend:8080"
---
apiVersion: v1
kind: Service
metadata:
  name: frontend
  namespace: $EVALUATION_NAMESPACE
spec:
  selector:
    app: frontend
  ports:
  - port: 80
    targetPort: 80
---
# Backend application
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: $EVALUATION_NAMESPACE
  labels:
    app: backend
    tier: api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
        tier: api
    spec:
      containers:
      - name: backend
        image: httpd:2.4
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        env:
        - name: DATABASE_URL
          value: "http://database:5432"
---
apiVersion: v1
kind: Service
metadata:
  name: backend
  namespace: $EVALUATION_NAMESPACE
spec:
  selector:
    app: backend
  ports:
  - port: 8080
    targetPort: 80
---
# Database application
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database
  namespace: $EVALUATION_NAMESPACE
  labels:
    app: database
    tier: data
spec:
  replicas: 1
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
        tier: data
    spec:
      containers:
      - name: database
        image: postgres:13
        ports:
        - containerPort: 5432
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        env:
        - name: POSTGRES_DB
          value: "testdb"
        - name: POSTGRES_USER
          value: "testuser"
        - name: POSTGRES_PASSWORD
          value: "testpass"
---
apiVersion: v1
kind: Service
metadata:
  name: database
  namespace: $EVALUATION_NAMESPACE
spec:
  selector:
    app: database
  ports:
  - port: 5432
    targetPort: 5432
TEST_APPS_EOF
    
    # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹
    kubectl wait --for=condition=available deployment/frontend -n $EVALUATION_NAMESPACE --timeout=300s
    kubectl wait --for=condition=available deployment/backend -n $EVALUATION_NAMESPACE --timeout=300s
    kubectl wait --for=condition=available deployment/database -n $EVALUATION_NAMESPACE --timeout=300s
    
    log "âœ… Evaluation environment ÑĞ¾Ğ·Ğ´Ğ°Ğ½"
}

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ baseline performance testing
run_baseline_performance_test() {
    log "ğŸ“Š Ğ—Ğ°Ğ¿ÑƒÑĞº baseline performance test"
    
    local baseline_report="/tmp/baseline-performance-$TIMESTAMP.json"
    
    # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ service IPs
    local frontend_ip=$(kubectl get svc frontend -n $EVALUATION_NAMESPACE -o jsonpath='{.spec.clusterIP}')
    local backend_ip=$(kubectl get svc backend -n $EVALUATION_NAMESPACE -o jsonpath='{.spec.clusterIP}')
    
    # Ğ—Ğ°Ğ¿ÑƒÑĞº performance tests
    cat > $baseline_report << BASELINE_REPORT_EOF
{
  "timestamp": "$(date -Iseconds)",
  "test_type": "baseline_without_service_mesh",
  "cluster": "$(kubectl config current-context)",
  "performance_metrics": {
    "frontend_response_time": {
$(kubectl run perf-test-frontend --image=busybox --rm -i --restart=Never --quiet -- sh -c "
time for i in \$(seq 1 10); do
  wget -q -O- http://$frontend_ip/ > /dev/null 2>&1
done" 2>&1 | grep real | awk '{print "      \"average_time\": \"" $2 "\""}')
    },
    "backend_response_time": {
$(kubectl run perf-test-backend --image=busybox --rm -i --restart=Never --quiet -- sh -c "
time for i in \$(seq 1 10); do
  wget -q -O- http://$backend_ip/ > /dev/null 2>&1
done" 2>&1 | grep real | awk '{print "      \"average_time\": \"" $2 "\""}')
    },
    "resource_usage": {
      "total_cpu_usage": "$(kubectl top pods -n $EVALUATION_NAMESPACE --no-headers | awk '{sum+=$2} END {print sum "m"}' | sed 's/m$//' || echo "0")m",
      "total_memory_usage": "$(kubectl top pods -n $EVALUATION_NAMESPACE --no-headers | awk '{sum+=$3} END {print sum "Mi"}' | sed 's/Mi$//' || echo "0")Mi",
      "pod_count": $(kubectl get pods -n $EVALUATION_NAMESPACE --no-headers | wc -l)
    }
  }
}
BASELINE_REPORT_EOF
    
    log "ğŸ“„ Baseline performance report: $baseline_report"
    
    # ĞšÑ€Ğ°Ñ‚ĞºĞ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    local pod_count=$(kubectl get pods -n $EVALUATION_NAMESPACE --no-headers | wc -l)
    local total_cpu=$(kubectl top pods -n $EVALUATION_NAMESPACE --no-headers 2>/dev/null | awk '{sum+=$2} END {print sum}' | sed 's/m$//' || echo "0")
    
    log "ğŸ¯ Baseline Performance:"
    log "  ğŸ“¦ Active pods: $pod_count"
    log "  âš¡ Total CPU: ${total_cpu}m"
    
    return 0
}

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ evaluation Istio
evaluate_istio_performance() {
    log "ğŸ” Evaluation Istio performance"
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ Istio
    if ! kubectl get namespace istio-system &>/dev/null; then
        log "âš ï¸ Istio Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº evaluation"
        return 1
    fi
    
    local istio_report="/tmp/istio-evaluation-$TIMESTAMP.json"
    
    # Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Istio injection
    kubectl label namespace $EVALUATION_NAMESPACE istio-injection=enabled --overwrite
    
    # ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº Ğ¿Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ sidecar injection
    kubectl rollout restart deployment/frontend -n $EVALUATION_NAMESPACE
    kubectl rollout restart deployment/backend -n $EVALUATION_NAMESPACE
    kubectl rollout restart deployment/database -n $EVALUATION_NAMESPACE
    
    # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸
    kubectl wait --for=condition=available deployment/frontend -n $EVALUATION_NAMESPACE --timeout=300s
    kubectl wait --for=condition=available deployment/backend -n $EVALUATION_NAMESPACE --timeout=300s
    kubectl wait --for=condition=available deployment/database -n $EVALUATION_NAMESPACE --timeout=300s
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° sidecar injection
    local sidecar_count=$(kubectl get pods -n $EVALUATION_NAMESPACE -o jsonpath='{.items[*].spec.containers[*].name}' | grep -o istio-proxy | wc -l)
    
    # Performance testing Ñ Istio
    local frontend_ip=$(kubectl get svc frontend -n $EVALUATION_NAMESPACE -o jsonpath='{.spec.clusterIP}')
    
    cat > $istio_report << ISTIO_REPORT_EOF
{
  "timestamp": "$(date -Iseconds)",
  "test_type": "istio_service_mesh",
  "cluster": "$(kubectl config current-context)",
  "istio_metrics": {
    "sidecar_injection": {
      "total_sidecars": $sidecar_count,
      "injection_successful": $([ $sidecar_count -gt 0 ] && echo "true" || echo "false")
    },
    "performance_metrics": {
      "frontend_response_time": {
$(kubectl run perf-test-istio --image=busybox --rm -i --restart=Never --quiet -- sh -c "
time for i in \$(seq 1 10); do
  wget -q -O- http://$frontend_ip/ > /dev/null 2>&1
done" 2>&1 | grep real | awk '{print "        \"average_time\": \"" $2 "\""}')
      },
      "resource_usage": {
        "total_cpu_usage": "$(kubectl top pods -n $EVALUATION_NAMESPACE --no-headers 2>/dev/null | awk '{sum+=$2} END {print sum "m"}' | sed 's/m$//' || echo "0")m",
        "total_memory_usage": "$(kubectl top pods -n $EVALUATION_NAMESPACE --no-headers 2>/dev/null | awk '{sum+=$3} END {print sum "Mi"}' | sed 's/Mi$//' || echo "0")Mi",
        "sidecar_overhead": {
          "cpu": "$(kubectl top pods -n $EVALUATION_NAMESPACE --containers 2>/dev/null | grep istio-proxy | awk '{sum+=$4} END {print sum "m"}' | sed 's/m$//' || echo "0")m",
          "memory": "$(kubectl top pods -n $EVALUATION_NAMESPACE --containers 2>/dev/null | grep istio-proxy | awk '{sum+=$5} END {print sum "Mi"}' | sed 's/Mi$//' || echo "0")Mi"
        }
      }
    },
    "security_features": {
      "mtls_enabled": $(kubectl get peerauthentication -n $EVALUATION_NAMESPACE --no-headers | wc -l | awk '{if($1>0) print "true"; else print "false"}'),
      "authorization_policies": $(kubectl get authorizationpolicies -n $EVALUATION_NAMESPACE --no-headers | wc -l)
    }
  }
}
ISTIO_REPORT_EOF
    
    log "ğŸ“„ Istio evaluation report: $istio_report"
    
    # ĞšÑ€Ğ°Ñ‚ĞºĞ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    local sidecar_cpu=$(kubectl top pods -n $EVALUATION_NAMESPACE --containers 2>/dev/null | grep istio-proxy | awk '{sum+=$4} END {print sum}' | sed 's/m$//' || echo "0")
    local sidecar_memory=$(kubectl top pods -n $EVALUATION_NAMESPACE --containers 2>/dev/null | grep istio-proxy | awk '{sum+=$5} END {print sum}' | sed 's/Mi$//' || echo "0")
    
    log "ğŸ¯ Istio Performance:"
    log "  ğŸ”— Sidecars injected: $sidecar_count"
    log "  âš¡ Sidecar CPU overhead: ${sidecar_cpu}m"
    log "  ğŸ’¾ Sidecar Memory overhead: ${sidecar_memory}Mi"
    
    return 0
}

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ evaluation Linkerd
evaluate_linkerd_performance() {
    log "ğŸ” Evaluation Linkerd performance"
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ Linkerd
    if ! kubectl get namespace linkerd &>/dev/null; then
        log "âš ï¸ Linkerd Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº evaluation"
        return 1
    fi
    
    local linkerd_report="/tmp/linkerd-evaluation-$TIMESTAMP.json"
    
    # ĞÑ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Istio injection
    kubectl label namespace $EVALUATION_NAMESPACE istio-injection-
    
    # Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Linkerd injection
    kubectl annotate namespace $EVALUATION_NAMESPACE linkerd.io/inject=enabled --overwrite
    
    # ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº Ğ¿Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ sidecar injection
    kubectl rollout restart deployment/frontend -n $EVALUATION_NAMESPACE
    kubectl rollout restart deployment/backend -n $EVALUATION_NAMESPACE
    kubectl rollout restart deployment/database -n $EVALUATION_NAMESPACE
    
    # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸
    kubectl wait --for=condition=available deployment/frontend -n $EVALUATION_NAMESPACE --timeout=300s
    kubectl wait --for=condition=available deployment/backend -n $EVALUATION_NAMESPACE --timeout=300s
    kubectl wait --for=condition=available deployment/database -n $EVALUATION_NAMESPACE --timeout=300s
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° sidecar injection
    local sidecar_count=$(kubectl get pods -n $EVALUATION_NAMESPACE -o jsonpath='{.items[*].spec.containers[*].name}' | grep -o linkerd-proxy | wc -l)
    
    # Performance testing Ñ Linkerd
    local frontend_ip=$(kubectl get svc frontend -n $EVALUATION_NAMESPACE -o jsonpath='{.spec.clusterIP}')
    
    cat > $linkerd_report << LINKERD_REPORT_EOF
{
  "timestamp": "$(date -Iseconds)",
  "test_type": "linkerd_service_mesh",
  "cluster": "$(kubectl config current-context)",
  "linkerd_metrics": {
    "sidecar_injection": {
      "total_sidecars": $sidecar_count,
      "injection_successful": $([ $sidecar_count -gt 0 ] && echo "true" || echo "false")
    },
    "performance_metrics": {
      "frontend_response_time": {
$(kubectl run perf-test-linkerd --image=busybox --rm -i --restart=Never --quiet -- sh -c "
time for i in \$(seq 1 10); do
  wget -q -O- http://$frontend_ip/ > /dev/null 2>&1
done" 2>&1 | grep real | awk '{print "        \"average_time\": \"" $2 "\""}')
      },
      "resource_usage": {
        "total_cpu_usage": "$(kubectl top pods -n $EVALUATION_NAMESPACE --no-headers 2>/dev/null | awk '{sum+=$2} END {print sum "m"}' | sed 's/m$//' || echo "0")m",
        "total_memory_usage": "$(kubectl top pods -n $EVALUATION_NAMESPACE --no-headers 2>/dev/null | awk '{sum+=$3} END {print sum "Mi"}' | sed 's/Mi$//' || echo "0")Mi",
        "sidecar_overhead": {
          "cpu": "$(kubectl top pods -n $EVALUATION_NAMESPACE --containers 2>/dev/null | grep linkerd-proxy | awk '{sum+=$4} END {print sum "m"}' | sed 's/m$//' || echo "0")m",
          "memory": "$(kubectl top pods -n $EVALUATION_NAMESPACE --containers 2>/dev/null | grep linkerd-proxy | awk '{sum+=$5} END {print sum "Mi"}' | sed 's/Mi$//' || echo "0")Mi"
        }
      }
    },
    "security_features": {
      "mtls_enabled": "true",
      "automatic_tls": "true"
    }
  }
}
LINKERD_REPORT_EOF
    
    log "ğŸ“„ Linkerd evaluation report: $linkerd_report"
    
    # ĞšÑ€Ğ°Ñ‚ĞºĞ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    local sidecar_cpu=$(kubectl top pods -n $EVALUATION_NAMESPACE --containers 2>/dev/null | grep linkerd-proxy | awk '{sum+=$4} END {print sum}' | sed 's/m$//' || echo "0")
    local sidecar_memory=$(kubectl top pods -n $EVALUATION_NAMESPACE --containers 2>/dev/null | grep linkerd-proxy | awk '{sum+=$5} END {print sum}' | sed 's/Mi$//' || echo "0")
    
    log "ğŸ¯ Linkerd Performance:"
    log "  ğŸ”— Sidecars injected: $sidecar_count"
    log "  âš¡ Sidecar CPU overhead: ${sidecar_cpu}m"
    log "  ğŸ’¾ Sidecar Memory overhead: ${sidecar_memory}Mi"
    
    return 0
}

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ comparison report
create_comparison_report() {
    log "ğŸ“‹ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ comprehensive comparison report"
    
    local comparison_report="/tmp/service-mesh-comparison-$TIMESTAMP.json"
    
    cat > $comparison_report << COMPARISON_REPORT_EOF
{
  "report_metadata": {
    "timestamp": "$(date -Iseconds)",
    "cluster": "$(kubectl config current-context)",
    "evaluation_namespace": "$EVALUATION_NAMESPACE",
    "kubernetes_version": "$(kubectl version --short 2>/dev/null | grep Server | awk '{print $3}' || echo "unknown")"
  },
  "service_mesh_comparison": {
    "available_solutions": {
      "istio": $(kubectl get namespace istio-system &>/dev/null && echo "true" || echo "false"),
      "linkerd": $(kubectl get namespace linkerd &>/dev/null && echo "true" || echo "false"),
      "consul": $(kubectl get namespace consul &>/dev/null && echo "true" || echo "false"),
      "cilium": $(kubectl get pods -n kube-system -l k8s-app=cilium --no-headers | wc -l | awk '{if($1>0) print "true"; else print "false"}')
    },
    "cluster_characteristics": {
      "total_nodes": $(kubectl get nodes --no-headers | wc -l),
      "total_cpu_cores": "$(kubectl get nodes -o jsonpath='{.items[*].status.capacity.cpu}' | tr ' ' '\n' | awk '{sum += $1} END {print sum}')cores",
      "total_memory": "$(kubectl get nodes -o jsonpath='{.items[*].status.capacity.memory}' | tr ' ' '\n' | sed 's/Ki$//' | awk '{sum += $1} END {print sum/1024/1024 "GB"}')",
      "cni_plugin": "$(kubectl get pods -n kube-system -o jsonpath='{.items[*].spec.containers[*].image}' | grep -o -E '(calico|cilium|flannel|weave)' | head -1 || echo "unknown")"
    },
    "recommendations": {
      "for_performance": "Linkerd Ğ¸Ğ»Ğ¸ Cilium",
      "for_features": "Istio",
      "for_simplicity": "Linkerd Ğ¸Ğ»Ğ¸ AWS App Mesh",
      "for_aws_workloads": "AWS App Mesh",
      "for_hashicorp_ecosystem": "Consul Connect",
      "for_network_focus": "Cilium Service Mesh"
    }
  },
  "evaluation_summary": {
    "baseline_established": $([ -f "/tmp/baseline-performance-$TIMESTAMP.json" ] && echo "true" || echo "false"),
    "istio_evaluated": $([ -f "/tmp/istio-evaluation-$TIMESTAMP.json" ] && echo "true" || echo "false"),
    "linkerd_evaluated": $([ -f "/tmp/linkerd-evaluation-$TIMESTAMP.json" ] && echo "true" || echo "false"),
    "next_steps": [
      "Review performance reports",
      "Consider feature requirements",
      "Evaluate operational complexity",
      "Plan pilot deployment"
    ]
  }
}
COMPARISON_REPORT_EOF
    
    log "ğŸ“„ Comprehensive comparison report: $comparison_report"
    
    # Summary
    local available_meshes=0
    kubectl get namespace istio-system &>/dev/null && available_meshes=$((available_meshes + 1))
    kubectl get namespace linkerd &>/dev/null && available_meshes=$((available_meshes + 1))
    kubectl get namespace consul &>/dev/null && available_meshes=$((available_meshes + 1))
    
    log "ğŸ¯ Service Mesh Comparison Summary:"
    log "  ğŸ“Š Available meshes: $available_meshes"
    log "  ğŸ¥ Cluster ready for service mesh"
    log "  ğŸ“‹ Evaluation reports generated"
    
    return 0
}

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ cleanup
cleanup_evaluation_environment() {
    log "ğŸ§¹ Cleanup evaluation environment"
    
    # Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ test namespace
    kubectl delete namespace $EVALUATION_NAMESPACE --ignore-not-found
    
    # Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ temporary reports
    rm -f /tmp/*-evaluation-$TIMESTAMP.json
    rm -f /tmp/cluster-readiness-$TIMESTAMP.json
    rm -f /tmp/service-mesh-comparison-$TIMESTAMP.json
    
    log "âœ… Cleanup Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½"
}

# ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ
main() {
    case "$1" in
        readiness)
            analyze_cluster_readiness
            ;;
        setup)
            create_evaluation_environment
            ;;
        baseline)
            run_baseline_performance_test
            ;;
        istio)
            evaluate_istio_performance
            ;;
        linkerd)
            evaluate_linkerd_performance
            ;;
        report)
            create_comparison_report
            ;;
        cleanup)
            cleanup_evaluation_environment
            ;;
        full)
            log "ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ service mesh evaluation"
            analyze_cluster_readiness
            create_evaluation_environment
            run_baseline_performance_test
            evaluate_istio_performance
            evaluate_linkerd_performance
            create_comparison_report
            log "ğŸ‰ Service mesh evaluation Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½!"
            ;;
        *)
            echo "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: $0 {readiness|setup|baseline|istio|linkerd|report|cleanup|full}"
            echo "  readiness - ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°"
            echo "  setup     - Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ evaluation environment"
            echo "  baseline  - Baseline performance test"
            echo "  istio     - Evaluation Istio performance"
            echo "  linkerd   - Evaluation Linkerd performance"
            echo "  report    - Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ comparison report"
            echo "  cleanup   - Cleanup evaluation environment"
            echo "  full      - ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ evaluation"
            exit 1
            ;;
    esac
}

# ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
trap 'log "âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ² service mesh evaluator"; exit 1' ERR

# Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸
main "$@"
EOF

chmod +x service-mesh-evaluator.sh
```

## ğŸ“Š **Service Mesh Decision Matrix:**

| ĞšÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ | Istio | Linkerd | Consul Connect | AWS App Mesh | Cilium |
|----------|-------|---------|----------------|--------------|--------|
| **Performance** | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| **Features** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ |
| **Complexity** | â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­ |
| **Security** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| **Observability** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­ |
| **Ecosystem** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ |

## ğŸ—ï¸ **ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° comparison Ğ² HA ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğµ:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Service Mesh Comparison Architecture           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Evaluation Environment (mesh-evaluation namespace)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚ â”‚
â”‚  â”‚ â”‚Frontend â”‚    â”‚Backend  â”‚    â”‚Database â”‚              â”‚ â”‚
â”‚  â”‚ â”‚(nginx)  â”‚â—„â”€â”€â–¶â”‚(httpd)  â”‚â—„â”€â”€â–¶â”‚(postgres)              â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Service Mesh Options Testing                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Istio     â”‚  Linkerd    â”‚   Consul    â”‚   Cilium    â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ Envoy   â”‚ â”œâ”€â”€ Rust    â”‚ â”œâ”€â”€ Envoy   â”‚ â”œâ”€â”€ eBPF    â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ Istiod  â”‚ â”œâ”€â”€ Control â”‚ â”œâ”€â”€ Agent   â”‚ â”œâ”€â”€ Agent   â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ Rich    â”‚ â”œâ”€â”€ Simple  â”‚ â”œâ”€â”€ Vault   â”‚ â”œâ”€â”€ Network â”‚  â”‚
â”‚  â”‚ â””â”€â”€ Complex â”‚ â””â”€â”€ Fast    â”‚ â””â”€â”€ Multi   â”‚ â””â”€â”€ Focus   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Performance Metrics Collection                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â”œâ”€â”€ Latency Measurement                                 â”‚ â”‚
â”‚  â”‚ â”œâ”€â”€ Resource Usage Tracking                             â”‚ â”‚
â”‚  â”‚ â”œâ”€â”€ Sidecar Overhead Analysis                           â”‚ â”‚
â”‚  â”‚ â”œâ”€â”€ Feature Completeness Assessment                     â”‚ â”‚
â”‚  â”‚ â””â”€â”€ Operational Complexity Evaluation                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ **Best Practices Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° service mesh:**

### **1. Performance Requirements**
- **High Performance**: Linkerd Ğ¸Ğ»Ğ¸ Cilium
- **Balanced Performance**: Consul Connect
- **Feature-rich**: Istio (Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹)

### **2. Operational Complexity**
- **Simple Operations**: Linkerd Ğ¸Ğ»Ğ¸ AWS App Mesh
- **Advanced Operations**: Istio
- **Hybrid Environments**: Consul Connect

### **3. Security Requirements**
- **Zero-trust**: Istio Ğ¸Ğ»Ğ¸ Consul Connect
- **Automatic mTLS**: Linkerd
- **Policy Enforcement**: Istio

### **4. Ecosystem Integration**
- **Kubernetes-native**: Istio Ğ¸Ğ»Ğ¸ Linkerd
- **HashiCorp stack**: Consul Connect
- **AWS workloads**: AWS App Mesh
- **Network focus**: Cilium

**Service mesh comparison Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ data-driven Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ² HA ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğµ!**
