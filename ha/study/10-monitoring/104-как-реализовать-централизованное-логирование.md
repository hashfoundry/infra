# 104. –ö–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

## üéØ **–ö–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ**

**–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ** –≤ Kubernetes - —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –¥–ª—è —Å–±–æ—Ä–∞, –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤ —Å–æ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞. –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –µ–¥–∏–Ω—É—é —Ç–æ—á–∫—É –¥–æ—Å—Ç—É–ø–∞ –∫ –ª–æ–≥–∞–º –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞, –æ—Ç–ª–∞–¥–∫–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º.

## üèóÔ∏è **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:**

### **1. –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã:**
- **Log Collectors** - —Å–±–æ—Ä—â–∏–∫–∏ –ª–æ–≥–æ–≤ (Fluentd, Fluent Bit, Filebeat)
- **Log Aggregators** - –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã –ª–æ–≥–æ–≤ (Elasticsearch, Loki)
- **Log Visualization** - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (Kibana, Grafana)
- **Log Storage** - —Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤ (Elasticsearch, S3, GCS)

### **2. –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–±–æ—Ä–∞:**
- **DaemonSet Pattern** - –∞–≥–µ–Ω—Ç –Ω–∞ –∫–∞–∂–¥–æ–º —É–∑–ª–µ
- **Sidecar Pattern** - –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä-—Å–±–æ—Ä—â–∏–∫ –≤ –∫–∞–∂–¥–æ–º Pod
- **Centralized Pattern** - —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–±–æ—Ä—â–∏–∫

## üìä **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –≤–∞—à–µ–≥–æ HA –∫–ª–∞—Å—Ç–µ—Ä–∞:**

### **1. –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:**
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
kubectl get pods --all-namespaces | grep -E "(fluentd|fluent-bit|elasticsearch|kibana|loki|grafana)"
```

### **2. –°–æ–∑–¥–∞–Ω–∏–µ comprehensive centralized logging toolkit:**
```bash
# –°–æ–∑–¥–∞—Ç—å —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
cat << 'EOF' > kubernetes-centralized-logging-toolkit.sh
#!/bin/bash

echo "=== Kubernetes Centralized Logging Implementation Toolkit ==="
echo "Comprehensive centralized logging setup for HashFoundry HA cluster"
echo

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
analyze_current_logging_state() {
    echo "=== Current Logging State Analysis ==="
    
    echo "1. Existing Logging Components:"
    echo "=============================="
    kubectl get pods --all-namespaces -o custom-columns="NAMESPACE:.metadata.namespace,NAME:.metadata.name,STATUS:.status.phase" | grep -E "(fluentd|fluent-bit|elasticsearch|kibana|loki|grafana|logstash)" || echo "No logging components found"
    echo
    
    echo "2. Node Log Locations:"
    echo "====================="
    echo "Container logs: /var/log/containers/"
    echo "Pod logs: /var/log/pods/"
    echo "System logs: /var/log/"
    echo "Kubelet logs: journalctl -u kubelet"
    echo
    
    echo "3. Current Log Drivers:"
    echo "======================"
    kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.containerRuntimeVersion}' | tr ' ' '\n' | sort | uniq
    echo
    
    echo "4. Available Storage Classes:"
    echo "============================"
    kubectl get storageclass -o custom-columns="NAME:.metadata.name,PROVISIONER:.provisioner,RECLAIM-POLICY:.reclaimPolicy"
    echo
}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è ELK Stack (Elasticsearch, Logstash, Kibana)
create_elk_stack() {
    echo "=== Creating ELK Stack for Centralized Logging ==="
    
    # –°–æ–∑–¥–∞—Ç—å namespace –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    kubectl create namespace logging --dry-run=client -o yaml | kubectl apply -f -
    
    # Elasticsearch StatefulSet
    cat << ELASTICSEARCH_EOF | kubectl apply -f -
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    component: elasticsearch
    hashfoundry.io/logging-stack: "elk"
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
        component: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
        ports:
        - containerPort: 9200
          name: rest
          protocol: TCP
        - containerPort: 9300
          name: inter-node
          protocol: TCP
        env:
        - name: cluster.name
          value: "kubernetes-logging"
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: "elasticsearch-0.elasticsearch,elasticsearch-1.elasticsearch,elasticsearch-2.elasticsearch"
        - name: cluster.initial_master_nodes
          value: "elasticsearch-0,elasticsearch-1,elasticsearch-2"
        - name: ES_JAVA_OPTS
          value: "-Xms512m -Xmx512m"
        - name: xpack.security.enabled
          value: "false"
        - name: xpack.monitoring.collection.enabled
          value: "true"
        resources:
          requests:
            cpu: "100m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        readinessProbe:
          httpGet:
            scheme: HTTP
            path: /_cluster/health?local=true
            port: 9200
          initialDelaySeconds: 5
        livenessProbe:
          httpGet:
            scheme: HTTP
            path: /_cluster/health?local=true
            port: 9200
          initialDelaySeconds: 90
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: elasticsearch
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    component: elasticsearch
spec:
  selector:
    app: elasticsearch
  clusterIP: None
  ports:
  - port: 9200
    name: rest
  - port: 9300
    name: inter-node
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-client
  namespace: logging
  labels:
    app: elasticsearch
    component: elasticsearch
spec:
  selector:
    app: elasticsearch
  ports:
  - port: 9200
    name: rest
    targetPort: 9200
ELASTICSEARCH_EOF
    
    # Kibana Deployment
    cat << KIBANA_EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
    component: kibana
    hashfoundry.io/logging-stack: "elk"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
        component: kibana
    spec:
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:7.17.0
        ports:
        - containerPort: 5601
        env:
        - name: ELASTICSEARCH_HOSTS
          value: "http://elasticsearch-client:9200"
        - name: SERVER_NAME
          value: "kibana"
        - name: SERVER_BASEPATH
          value: ""
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
        readinessProbe:
          httpGet:
            path: /api/status
            port: 5601
          initialDelaySeconds: 30
        livenessProbe:
          httpGet:
            path: /api/status
            port: 5601
          initialDelaySeconds: 60
---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
    component: kibana
spec:
  selector:
    app: kibana
  ports:
  - port: 5601
    targetPort: 5601
    name: ui
KIBANA_EOF
    
    echo "‚úÖ ELK Stack components created"
    echo
}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è Fluentd DaemonSet
create_fluentd_daemonset() {
    echo "=== Creating Fluentd DaemonSet ==="
    
    # Fluentd ConfigMap
    cat << FLUENTD_CONFIG_EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: logging
  labels:
    app: fluentd
    component: fluentd
data:
  fluent.conf: |
    <source>
      @type tail
      @id in_tail_container_logs
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>
    
    <source>
      @type tail
      @id in_tail_minion
      path /var/log/salt/minion
      pos_file /var/log/fluentd-salt.pos
      tag salt
      <parse>
        @type regexp
        expression /^(?<time>[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?<severity>[^ \]]*) *\] (?<message>.*)$/
        time_format %Y-%m-%d %H:%M:%S
      </parse>
    </source>
    
    <source>
      @type tail
      @id in_tail_startupscript
      path /var/log/startupscript.log
      pos_file /var/log/fluentd-startupscript.log.pos
      tag startupscript
      <parse>
        @type syslog
      </parse>
    </source>
    
    <source>
      @type tail
      @id in_tail_docker
      path /var/log/docker.log
      pos_file /var/log/fluentd-docker.log.pos
      tag docker
      <parse>
        @type regexp
        expression /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
      </parse>
    </source>
    
    <source>
      @type tail
      @id in_tail_etcd
      path /var/log/etcd.log
      pos_file /var/log/fluentd-etcd.log.pos
      tag etcd
      <parse>
        @type none
      </parse>
    </source>
    
    <source>
      @type tail
      @id in_tail_kubelet
      multiline_flush_interval 5s
      path /var/log/kubelet.log
      pos_file /var/log/fluentd-kubelet.log.pos
      tag kubelet
      <parse>
        @type kubernetes
      </parse>
    </source>
    
    <source>
      @type tail
      @id in_tail_kube_proxy
      multiline_flush_interval 5s
      path /var/log/kube-proxy.log
      pos_file /var/log/fluentd-kube-proxy.log.pos
      tag kube-proxy
      <parse>
        @type kubernetes
      </parse>
    </source>
    
    <source>
      @type tail
      @id in_tail_kube_apiserver
      multiline_flush_interval 5s
      path /var/log/kube-apiserver.log
      pos_file /var/log/fluentd-kube-apiserver.log.pos
      tag kube-apiserver
      <parse>
        @type kubernetes
      </parse>
    </source>
    
    <source>
      @type tail
      @id in_tail_kube_controller_manager
      multiline_flush_interval 5s
      path /var/log/kube-controller-manager.log
      pos_file /var/log/fluentd-kube-controller-manager.log.pos
      tag kube-controller-manager
      <parse>
        @type kubernetes
      </parse>
    </source>
    
    <source>
      @type tail
      @id in_tail_kube_scheduler
      multiline_flush_interval 5s
      path /var/log/kube-scheduler.log
      pos_file /var/log/fluentd-kube-scheduler.log.pos
      tag kube-scheduler
      <parse>
        @type kubernetes
      </parse>
    </source>
    
    <source>
      @type tail
      @id in_tail_rescheduler
      multiline_flush_interval 5s
      path /var/log/rescheduler.log
      pos_file /var/log/fluentd-rescheduler.log.pos
      tag rescheduler
      <parse>
        @type kubernetes
      </parse>
    </source>
    
    <source>
      @type tail
      @id in_tail_glbc
      multiline_flush_interval 5s
      path /var/log/glbc.log
      pos_file /var/log/fluentd-glbc.log.pos
      tag glbc
      <parse>
        @type kubernetes
      </parse>
    </source>
    
    <source>
      @type tail
      @id in_tail_cluster_autoscaler
      multiline_flush_interval 5s
      path /var/log/cluster-autoscaler.log
      pos_file /var/log/fluentd-cluster-autoscaler.log.pos
      tag cluster-autoscaler
      <parse>
        @type kubernetes
      </parse>
    </source>
    
    <filter kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
      kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
      verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
      ca_file "#{ENV['KUBERNETES_CA_FILE']}"
      skip_labels "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_LABELS'] || 'false'}"
      skip_container_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_CONTAINER_METADATA'] || 'false'}"
      skip_master_url "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_MASTER_URL'] || 'false'}"
      skip_namespace_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_NAMESPACE_METADATA'] || 'false'}"
    </filter>
    
    <match **>
      @type elasticsearch
      @id out_es
      @log_level info
      include_tag_key true
      host "#{ENV['FLUENT_ELASTICSEARCH_HOST'] || 'elasticsearch-client'}"
      port "#{ENV['FLUENT_ELASTICSEARCH_PORT'] || '9200'}"
      path "#{ENV['FLUENT_ELASTICSEARCH_PATH'] || ''}"
      scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
      ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
      ssl_version "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERSION'] || 'TLSv1_2'}"
      user "#{ENV['FLUENT_ELASTICSEARCH_USER'] || use_default}"
      password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD'] || use_default}"
      reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'false'}"
      reconnect_on_error "#{ENV['FLUENT_ELASTICSEARCH_RECONNECT_ON_ERROR'] || 'true'}"
      reload_on_failure "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_ON_FAILURE'] || 'true'}"
      log_es_400_reason "#{ENV['FLUENT_ELASTICSEARCH_LOG_ES_400_REASON'] || 'false'}"
      logstash_prefix "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'logstash'}"
      logstash_dateformat "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_DATEFORMAT'] || '%Y.%m.%d'}"
      logstash_format "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_FORMAT'] || 'true'}"
      index_name "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_INDEX_NAME'] || 'logstash'}"
      type_name "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_TYPE_NAME'] || 'fluentd'}"
      <buffer>
        flush_thread_count "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_THREAD_COUNT'] || '8'}"
        flush_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL'] || '5s'}"
        chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '2M'}"
        queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '32'}"
        retry_max_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_RETRY_MAX_INTERVAL'] || '30'}"
        retry_forever true
      </buffer>
    </match>
FLUENTD_CONFIG_EOF
    
    # Fluentd DaemonSet
    cat << FLUENTD_DAEMONSET_EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
    component: fluentd
    hashfoundry.io/logging-stack: "elk"
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
        component: fluentd
    spec:
      serviceAccount: fluentd
      serviceAccountName: fluentd
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1.14-debian-elasticsearch7-1
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch-client"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: FLUENT_ELASTICSEARCH_SCHEME
          value: "http"
        - name: FLUENTD_SYSTEMD_CONF
          value: disable
        - name: FLUENT_CONTAINER_TAIL_EXCLUDE_PATH
          value: /var/log/containers/fluent*
        - name: FLUENT_CONTAINER_TAIL_PARSER_TYPE
          value: /^(?<time>.+) (?<stream>stdout|stderr)( (?<logtag>.))? (?<log>.*)$/
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluentd-config
          mountPath: /fluentd/etc
        - name: varlogpods
          mountPath: /var/log/pods
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: varlogpods
        hostPath:
          path: /var/log/pods
      - name: fluentd-config
        configMap:
          name: fluentd-config
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
  labels:
    app: fluentd
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - namespaces
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: logging
FLUENTD_DAEMONSET_EOF
    
    echo "‚úÖ Fluentd DaemonSet created"
    echo
}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è Loki + Promtail stack (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ ELK)
create_loki_stack() {
    echo "=== Creating Loki + Promtail Stack ==="
    
    # Loki Deployment
    cat << LOKI_EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: loki
  namespace: logging
  labels:
    app: loki
    component: loki
    hashfoundry.io/logging-stack: "loki"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
        component: loki
    spec:
      containers:
      - name: loki
        image: grafana/loki:2.8.0
        ports:
        - containerPort: 3100
          name: http-metrics
        args:
        - -config.file=/etc/loki/local-config.yaml
        volumeMounts:
        - name: loki-config
          mountPath: /etc/loki
        - name: loki-storage
          mountPath: /loki
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
        readinessProbe:
          httpGet:
            path: /ready
            port: 3100
          initialDelaySeconds: 45
        livenessProbe:
          httpGet:
            path: /ready
            port: 3100
          initialDelaySeconds: 45
      volumes:
      - name: loki-config
        configMap:
          name: loki-config
      - name: loki-storage
        emptyDir: {}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: logging
  labels:
    app: loki
data:
  local-config.yaml: |
    auth_enabled: false
    
    server:
      http_listen_port: 3100
      grpc_listen_port: 9096
    
    common:
      path_prefix: /loki
      storage:
        filesystem:
          chunks_directory: /loki/chunks
          rules_directory: /loki/rules
      replication_factor: 1
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: inmemory
    
    query_scheduler:
      max_outstanding_requests_per_tenant: 32768
    
    schema_config:
      configs:
        - from: 2020-10-24
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h
    
    ruler:
      alertmanager_url: http://localhost:9093
    
    analytics:
      reporting_enabled: false
---
apiVersion: v1
kind: Service
metadata:
  name: loki
  namespace: logging
  labels:
    app: loki
    component: loki
spec:
  selector:
    app: loki
  ports:
  - port: 3100
    targetPort: 3100
    name: http-metrics
LOKI_EOF
    
    # Promtail DaemonSet
    cat << PROMTAIL_EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: promtail
  namespace: logging
  labels:
    app: promtail
    component: promtail
    hashfoundry.io/logging-stack: "loki"
spec:
  selector:
    matchLabels:
      app: promtail
  template:
    metadata:
      labels:
        app: promtail
        component: promtail
    spec:
      serviceAccount: promtail
      containers:
      - name: promtail
        image: grafana/promtail:2.8.0
        args:
        - -config.file=/etc/promtail/config.yml
        - -client.url=http://loki:3100/loki/api/v1/push
        env:
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        volumeMounts:
        - name: config
          mountPath: /etc/promtail
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: varlogpods
          mountPath: /var/log/pods
          readOnly: true
        resources:
          requests:
            cpu: "50m"
            memory: "64Mi"
          limits:
            cpu: "200m"
            memory: "128Mi"
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      volumes:
      - name: config
        configMap:
          name: promtail-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: varlogpods
        hostPath:
          path: /var/log/pods
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
  namespace: logging
  labels:
    app: promtail
data:
  config.yml: |
    server:
      http_listen_port: 9080
      grpc_listen_port: 0
    
    positions:
      filename: /tmp/positions.yaml
    
    clients:
      - url: http://loki:3100/loki/api/v1/push
    
    scrape_configs:
    - job_name: containers
      static_configs:
      - targets:
          - localhost
        labels:
          job: containerlogs
          __path__: /var/log/containers/*log
      
      pipeline_stages:
      - json:
          expressions:
            output: log
            stream: stream
            attrs:
      - json:
          expressions:
            tag: attrs.tag
          source: attrs
      - regex:
          expression: (?P<pod_name>(?P<container_name>.+)-\w{8,10})-(?P<uid>\w{8,4}-\w{4}-\w{4}-\w{4}-\w{12})
          source: tag
      - timestamp:
          format: RFC3339Nano
          source: time
      - labels:
          stream:
          pod_name:
          container_name:
      - output:
          source: output
    
    - job_name: syslog
      static_configs:
      - targets:
          - localhost
        labels:
          job: syslog
          __path__: /var/log/syslog
    
    - job_name: kernlog
      static_configs:
      - targets:
          - localhost
        labels:
          job: kernlog
          __path__: /var/log/kern.log
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: promtail
  namespace: logging
  labels:
    app: promtail
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: promtail
  labels:
    app: promtail
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: promtail
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: promtail
subjects:
- kind: ServiceAccount
  name: promtail
  namespace: logging
PROMTAIL_EOF
    
    echo "‚úÖ Loki + Promtail stack created"
    echo
}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
create_logging_monitoring() {
    echo "=== Creating Logging Monitoring Tools ==="
    
    cat << LOGGING_MONITOR_EOF > logging-monitoring.sh
#!/bin/bash

echo "=== Centralized Logging Monitoring ==="
echo "Monitoring tools for centralized logging in HashFoundry HA cluster"
echo

# Function to check logging stack health
check_logging_health() {
    echo "1. Logging Stack Health Check:"
    echo "============================="
    
    echo "ELK Stack Components:"
    echo "--------------------"
    kubectl get pods -n logging -l hashfoundry.io/logging-stack=elk -o custom-columns="NAME:.metadata.name,STATUS:.status.phase,READY:.status.conditions[?(@.type=='Ready')].status,RESTARTS:.status.containerStatuses[0].restartCount" 2>/dev/null || echo "ELK stack not found"
    echo
    
    echo "Loki Stack Components:"
    echo "---------------------"
    kubectl get pods -n logging -l hashfoundry.io/logging-stack=loki -o custom-columns="NAME:.metadata.name,STATUS:.status.phase,READY:.status.conditions[?(@.type=='Ready')].status,RESTARTS:.status.containerStatuses[0].restartCount" 2>/dev/null || echo "Loki stack not found"
    echo
    
    echo "Log Collectors (DaemonSets):"
    echo "----------------------------"
    kubectl get daemonsets -n logging -o custom-columns="NAME:.metadata.name,DESIRED:.status.desiredNumberScheduled,CURRENT:.status.currentNumberScheduled,READY:.status.numberReady"
    echo
}

# Main function
main() {
    check_logging_health
}

main

LOGGING_MONITOR_EOF
    
    chmod +x logging-monitoring.sh
    echo "‚úÖ Logging monitoring tools created"
    echo
}

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
main() {
    case "$1" in
        "analyze")
            analyze_current_logging_state
            ;;
        "elk")
            create_elk_stack
            create_fluentd_daemonset
            ;;
        "loki")
            create_loki_stack
            ;;
        "monitor")
            create_logging_monitoring
            ;;
        "all"|"")
            analyze_current_logging_state
            echo "Choose logging stack:"
            echo "1. ELK Stack (Elasticsearch + Kibana + Fluentd)"
            echo "2. Loki Stack (Loki + Promtail)"
            read -p "Enter choice (1 or 2): " choice
            case $choice in
                1)
                    create_elk_stack
                    create_fluentd_daemonset
                    ;;
                2)
                    create_loki_stack
                    ;;
                *)
                    echo "Invalid choice. Creating both stacks."
                    create_elk_stack
                    create_fluentd_daemonset
                    create_loki_stack
                    ;;
            esac
            create_logging_monitoring
            ;;
        *)
            echo "Usage: $0 [action]"
            echo "Actions: analyze, elk, loki, monitor, all"
            ;;
    esac
}

main "$@"

EOF

chmod +x kubernetes-centralized-logging-toolkit.sh
./kubernetes-centralized-logging-toolkit.sh analyze
```

## üìã **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:**

### **ELK Stack:**
- **Elasticsearch** - —Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
- **Logstash/Fluentd** - —Å–±–æ—Ä –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞
- **Kibana** - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑

### **Loki Stack:**
- **Loki** - —Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤
- **Promtail** - —Å–±–æ—Ä –ª–æ–≥–æ–≤
- **Grafana** - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è

## üéØ **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–º–∞–Ω–¥—ã:**

### **–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ:**
```bash
# –°–æ–∑–¥–∞—Ç—å ELK Stack
./kubernetes-centralized-logging-toolkit.sh elk

# –°–æ–∑–¥–∞—Ç—å Loki Stack
./kubernetes-centralized-logging-toolkit.sh loki

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å
kubectl get pods -n logging
```

### **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**
```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
./logging-monitoring.sh

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
kubectl logs -n logging -l app=fluentd
kubectl logs -n logging -l app=promtail
```

**–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –æ—Ç–ª–∞–¥–∫–∏ Kubernetes –∫–ª–∞—Å—Ç–µ—Ä–∞!**
