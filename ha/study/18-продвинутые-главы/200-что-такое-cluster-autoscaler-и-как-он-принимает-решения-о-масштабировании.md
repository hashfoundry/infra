# 200. Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Cluster Autoscaler Ğ¸ ĞºĞ°Ğº Ğ¾Ğ½ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸?

## ğŸ¯ **Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Cluster Autoscaler?**

**Cluster Autoscaler** â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Kubernetes, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° (Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ»Ğ¸ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ ÑƒĞ·Ğ»Ñ‹) Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…. ĞĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ pending Pod'Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.

## ğŸ—ï¸ **ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Cluster Autoscaler:**

### **1. Scale-Up (Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°)**
- ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ pending Pod'Ğ¾Ğ² Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²
- ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Node'Ğ¾Ğ²
- Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ cloud provider API

### **2. Scale-Down (Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°)**
- ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ½ĞµĞ´Ğ¾Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Node'Ğ¾Ğ²
- Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒÑÑ‚Ñ‹Ñ… Node'Ğ¾Ğ²
- Ğ¡Ğ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ PodDisruptionBudgets

### **3. Resource Optimization**
- ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ
- Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²
- Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ availability Ğ¸ cost

### ğŸ—ï¸ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Cluster Autoscaler

#### 1. **Ğ¡Ñ…ĞµĞ¼Ğ° Cluster Autoscaler Architecture**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Cluster Autoscaler Architecture               â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                   Pod Scheduling                       â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚   Pending   â”‚    â”‚  Scheduled  â”‚    â”‚ Unscheduled â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    Pods     â”‚â”€â”€â”€â–¶â”‚    Pods     â”‚â”€â”€â”€â–¶â”‚    Pods     â”‚ â”‚ â”‚
â”‚  â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                              â”‚
â”‚                              â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                Cluster Autoscaler                      â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚   Scale-Up  â”‚    â”‚ Scale-Down  â”‚    â”‚   Node      â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   Logic     â”‚â”€â”€â”€â–¶â”‚   Logic     â”‚â”€â”€â”€â–¶â”‚ Management  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                              â”‚
â”‚                              â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                 Decision Engine                        â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚  Resource   â”‚    â”‚   Policy    â”‚    â”‚  Simulation â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ Monitoring  â”‚â”€â”€â”€â–¶â”‚ Evaluation  â”‚â”€â”€â”€â–¶â”‚  & Testing  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                              â”‚
â”‚                              â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                Cloud Provider API                     â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚    AWS      â”‚    â”‚   Azure     â”‚    â”‚    GCP      â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ Auto Scalingâ”‚â”€â”€â”€â–¶â”‚ Scale Sets  â”‚â”€â”€â”€â–¶â”‚ Instance    â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   Groups    â”‚    â”‚             â”‚    â”‚   Groups    â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                              â”‚
â”‚                              â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                  Node Pool                             â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚   Worker    â”‚    â”‚   Worker    â”‚    â”‚   Worker    â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   Node 1    â”‚â”€â”€â”€â–¶â”‚   Node 2    â”‚â”€â”€â”€â–¶â”‚   Node N    â”‚ â”‚ â”‚
â”‚  â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. **Scaling Decision Logic**
```yaml
# Cluster Autoscaler Decision Logic
scaling_decisions:
  scale_up_triggers:
    pending_pods:
      condition: "Pods cannot be scheduled due to insufficient resources"
      evaluation_period: "10 seconds"
      requirements:
        - "Pod has been pending for more than 10 seconds"
        - "No suitable node exists for pod scheduling"
        - "Adding a node would enable pod scheduling"
    
    resource_pressure:
      cpu_threshold: "80%"
      memory_threshold: "80%"
      evaluation_window: "5 minutes"
      
    node_group_constraints:
      min_size: "Minimum nodes per node group"
      max_size: "Maximum nodes per node group"
      desired_capacity: "Target number of nodes"

  scale_down_triggers:
    node_underutilization:
      cpu_threshold: "50%"
      memory_threshold: "50%"
      evaluation_period: "10 minutes"
      grace_period: "10 minutes"
    
    node_emptiness:
      condition: "Node has no non-DaemonSet pods"
      evaluation_period: "10 minutes"
      
    cost_optimization:
      target: "Minimize cluster cost while maintaining SLA"
      strategy: "Remove least utilized nodes first"

  scaling_policies:
    scale_up_policy:
      max_nodes_per_scale: "10"
      scale_up_delay: "0 seconds"
      stabilization_window: "3 minutes"
      
    scale_down_policy:
      max_nodes_per_scale: "1"
      scale_down_delay: "10 minutes"
      stabilization_window: "5 minutes"
      
    node_group_priorities:
      strategy: "Priority-based scaling"
      factors:
        - "Cost per hour"
        - "Resource efficiency"
        - "Availability zone distribution"

  decision_algorithm:
    simulation_phase:
      steps:
        - "Identify unschedulable pods"
        - "Find suitable node groups"
        - "Simulate pod placement"
        - "Calculate resource requirements"
        - "Estimate scaling impact"
    
    validation_phase:
      checks:
        - "Node group limits"
        - "Budget constraints"
        - "Availability zone balance"
        - "Taints and tolerations"
        - "Node selectors and affinity"
    
    execution_phase:
      actions:
        - "Call cloud provider API"
        - "Wait for node readiness"
        - "Update cluster state"
        - "Monitor scaling results"

  constraints_and_policies:
    node_constraints:
      unremovable_nodes:
        - "Nodes with local storage pods"
        - "Nodes with non-replicated pods"
        - "Nodes with PodDisruptionBudget violations"
        - "Nodes with system pods (non-DaemonSet)"
      
      removable_nodes:
        - "Nodes with only DaemonSet pods"
        - "Empty nodes (no pods)"
        - "Nodes with replicated pods that can be rescheduled"
    
    scaling_constraints:
      rate_limiting:
        max_scale_up_rate: "10 nodes per minute"
        max_scale_down_rate: "1 node per minute"
        
      resource_limits:
        max_cluster_size: "1000 nodes"
        max_node_group_size: "100 nodes"
        
      cost_controls:
        budget_limits: "Monthly spending cap"
        instance_types: "Allowed instance types"
```

## ğŸ“Š **ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ğ²Ğ°ÑˆĞµĞ³Ğ¾ HA ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°:**

### **1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° DigitalOcean Auto-scaling:**
```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ¸ node pools Ğ² DigitalOcean
doctl kubernetes cluster get hashfoundry-ha

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° node pool Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞº
doctl kubernetes cluster node-pool list hashfoundry-ha

# Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğµ Node'Ñ‹ Ğ² ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğµ
kubectl get nodes -o wide

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° auto-scaling Ğ¼ĞµÑ‚Ğ¾Ğº Ğ½Ğ° Node'Ğ°Ñ…
kubectl get nodes --show-labels | grep "doks.digitalocean.com"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° pending Pod'Ğ¾Ğ²
kubectl get pods --all-namespaces --field-selector=status.phase=Pending
```

### **2. ArgoCD Ğ¸ Monitoring Ğ½Ğ° Node'Ğ°Ñ…:**
```bash
# ArgoCD Pod'Ñ‹ Ğ¸ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ
kubectl get pods -n argocd -o wide

# Monitoring Pod'Ñ‹ Ğ¸ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ
kubectl get pods -n monitoring -o wide

# Resource requests ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²
kubectl describe deployment argocd-server -n argocd | grep -A 10 "Requests"
kubectl describe deployment prometheus-server -n monitoring | grep -A 10 "Requests"

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Node'Ğ¾Ğ²
kubectl top nodes
kubectl describe nodes | grep -A 10 "Allocated resources"
```

### **3. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ auto-scaling:**
```bash
# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾ĞµĞ¼ĞºĞ¾Ğ³Ğ¾ Deployment
kubectl create deployment scale-test --image=nginx --replicas=1

# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… resource requests
kubectl patch deployment scale-test -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","resources":{"requests":{"cpu":"1500m","memory":"2Gi"}}}]}}}}'

# ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ pressure
kubectl scale deployment scale-test --replicas=8

# ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ auto-scaling Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°
watch "kubectl get nodes; echo '---'; kubectl get pods -l app=scale-test"

# ĞÑ‡Ğ¸ÑÑ‚ĞºĞ°
kubectl delete deployment scale-test
```

### **4. ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ² Grafana:**
```bash
# Port forward Ğº Grafana
kubectl port-forward svc/grafana -n monitoring 3000:80

# ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ auto-scaling:
# - node_cpu_utilization
# - node_memory_utilization  
# - kube_node_status_ready
# - kube_pod_status_phase{phase="Pending"}
```

### ğŸ› ï¸ Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Cluster Autoscaler

#### 1. **Cluster Autoscaler Deployment**
```yaml
# cluster-autoscaler.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
        name: cluster-autoscaler
        resources:
          limits:
            cpu: 100m
            memory: 300Mi
          requests:
            cpu: 100m
            memory: 300Mi
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=digitalocean
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=digitalocean:tag=k8s-cluster-autoscaler-enabled
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false
        - --scale-down-enabled=true
        - --scale-down-delay-after-add=10m
        - --scale-down-unneeded-time=10m
        - --scale-down-utilization-threshold=0.5
        - --max-node-provision-time=15m
        - --max-nodes-total=10
        - --cores-total=0:100
        - --memory-total=0:1000
        env:
        - name: DO_TOKEN
          valueFrom:
            secretKeyRef:
              name: cluster-autoscaler-secret
              key: token
        - name: CLUSTER_NAME
          value: "hashfoundry-ha"
        volumeMounts:
        - name: ssl-certs
          mountPath: /etc/ssl/certs/ca-certificates.crt
          readOnly: true
        imagePullPolicy: Always
      volumes:
      - name: ssl-certs
        hostPath:
          path: /etc/ssl/certs/ca-certificates.crt

---
# ServiceAccount for cluster autoscaler
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system

---
# ClusterRole for cluster autoscaler
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""]
  resources: ["events", "endpoints"]
  verbs: ["create", "patch"]
- apiGroups: [""]
  resources: ["pods/eviction"]
  verbs: ["create"]
- apiGroups: [""]
  resources: ["pods/status"]
  verbs: ["update"]
- apiGroups: [""]
  resources: ["endpoints"]
  resourceNames: ["cluster-autoscaler"]
  verbs: ["get", "update"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["watch", "list", "get", "update"]
- apiGroups: [""]
  resources: ["pods", "services", "replicationcontrollers", "persistentvolumeclaims", "persistentvolumes"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["extensions"]
  resources: ["replicasets", "daemonsets"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["watch", "list"]
- apiGroups: ["apps"]
  resources: ["statefulsets", "replicasets", "daemonsets"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses", "csinodes"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["batch", "extensions"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "patch"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["create"]
- apiGroups: ["coordination.k8s.io"]
  resourceNames: ["cluster-autoscaler"]
  resources: ["leases"]
  verbs: ["get", "update"]

---
# ClusterRoleBinding for cluster autoscaler
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
- kind: ServiceAccount
  name: cluster-autoscaler
  namespace: kube-system

---
# Secret for cloud provider credentials
apiVersion: v1
kind: Secret
metadata:
  name: cluster-autoscaler-secret
  namespace: kube-system
type: Opaque
data:
  token: ZG9wX3YxX2JjODg1OTM1OTY0MTRkN2E2YWE1MzVmODQ4OTcyZTFiMGViZmE1NDBhNDVjMWZjMmE5MWU1MzU2YjVmOWEyYzI=  # base64 encoded DO token
```

#### 2. **Node Pool Configuration**
```yaml
# node-pool-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-config
  namespace: kube-system
data:
  nodes.max: "10"
  nodes.min: "3"
  scale-down-enabled: "true"
  scale-down-delay-after-add: "10m"
  scale-down-unneeded-time: "10m"
  scale-down-utilization-threshold: "0.5"
  skip-nodes-with-local-storage: "false"
  skip-nodes-with-system-pods: "false"
  expander: "least-waste"
  balance-similar-node-groups: "true"
  max-node-provision-time: "15m"

---
# Node pool labels for autoscaling
apiVersion: v1
kind: Node
metadata:
  name: worker-node-1
  labels:
    node-pool: "ha-worker-pool"
    k8s-cluster-autoscaler-enabled: "true"
    k8s-cluster-autoscaler-hashfoundry-ha: "owned"
    instance-type: "s-2vcpu-4gb"
    topology.kubernetes.io/zone: "fra1-1"
spec:
  # Node specification
```

#### 3. **Test Workloads Ğ´Ğ»Ñ Autoscaling**
```yaml
# test-scaling-workload.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource-intensive-app
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: resource-intensive-app
  template:
    metadata:
      labels:
        app: resource-intensive-app
    spec:
      containers:
      - name: cpu-intensive
        image: busybox
        command: ["/bin/sh"]
        args:
        - -c
        - |
          echo "Starting CPU intensive workload..."
          while true; do
            for i in $(seq 1 4); do
              dd if=/dev/zero of=/dev/null &
            done
            sleep 10
            killall dd
            sleep 5
          done
        resources:
          requests:
            cpu: 1000m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 2Gi

---
# Horizontal Pod Autoscaler for testing
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: resource-intensive-app-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: resource-intensive-app
  minReplicas: 1
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

---
# Load generator for testing
apiVersion: batch/v1
kind: Job
metadata:
  name: load-generator
  namespace: default
spec:
  parallelism: 5
  completions: 10
  template:
    metadata:
      labels:
        app: load-generator
    spec:
      restartPolicy: Never
      containers:
      - name: load-generator
        image: busybox
        command: ["/bin/sh"]
        args:
        - -c
        - |
          echo "Generating load..."
          # Generate CPU load
          for i in $(seq 1 300); do
            echo "Load iteration $i"
            dd if=/dev/zero of=/tmp/test bs=1M count=100 2>/dev/null
            rm /tmp/test
            sleep 1
          done
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
```

### ğŸ”§ Ğ£Ñ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Cluster Autoscaler

#### Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° autoscaling:
```bash
#!/bin/bash
# monitor-cluster-autoscaler.sh

echo "ğŸ” Monitoring Cluster Autoscaler"

# Monitor cluster autoscaler status
monitor_autoscaler_status() {
    echo "=== Cluster Autoscaler Status ==="
    
    # Check if autoscaler is running
    autoscaler_pod=$(kubectl get pods -n kube-system -l app=cluster-autoscaler -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
    
    if [ -n "$autoscaler_pod" ]; then
        echo "âœ… Cluster Autoscaler pod: $autoscaler_pod"
        
        # Check pod status
        status=$(kubectl get pod $autoscaler_pod -n kube-system -o jsonpath='{.status.phase}')
        echo "Pod status: $status"
        
        # Check recent logs
        echo "--- Recent Autoscaler Logs ---"
        kubectl logs -n kube-system $autoscaler_pod --tail=10 | grep -E "(scale|node|decision)"
    else
        echo "âŒ Cluster Autoscaler pod not found"
        return 1
    fi
}

# Monitor node status and capacity
monitor_node_status() {
    echo "=== Node Status and Capacity ==="
    
    # Show current nodes
    echo "--- Current Nodes ---"
    kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions[-1].type,ROLES:.metadata.labels.node-role\\.kubernetes\\.io/worker,AGE:.metadata.creationTimestamp,INSTANCE-TYPE:.metadata.labels.node\\.kubernetes\\.io/instance-type
    
    # Show node resource usage
    echo "--- Node Resource Usage ---"
    kubectl top nodes 2>/dev/null || echo "Metrics server not available"
    
    # Show node capacity and allocatable
    echo "--- Node Capacity ---"
    kubectl describe nodes | grep -A 5 "Capacity:\|Allocatable:" | head -20
}

# Monitor pending pods
monitor_pending_pods() {
    echo "=== Pending Pods Analysis ==="
    
    # Get pending pods
    pending_pods=$(kubectl get pods --all-namespaces --field-selector=status.phase=Pending --no-headers)
    
    if [ -n "$pending_pods" ]; then
        echo "âš ï¸  Found pending pods:"
        echo "$pending_pods"
        
        # Analyze why pods are pending
        echo "--- Pending Pod Details ---"
        kubectl get pods --all-namespaces --field-selector=status.phase=Pending -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,REASON:.status.conditions[0].reason,MESSAGE:.status.conditions[0].message
    else
        echo "âœ… No pending pods found"
    fi
}

# Monitor scaling events
monitor_scaling_events() {
    echo "=== Scaling Events ==="
    
    # Get recent scaling events
    echo "--- Recent Scaling Events ---"
    kubectl get events --all-namespaces --sort-by='.lastTimestamp' | grep -i "scale\|autoscaler\|node" | tail -10
    
    # Check for specific autoscaler events
    echo "--- Autoscaler Specific Events ---"
    kubectl get events --all-namespaces --field-selector source=cluster-autoscaler --sort-by='.lastTimestamp' | tail -5
}

# Monitor resource pressure
monitor_resource_pressure() {
    echo "=== Resource Pressure Analysis ==="
    
    # Calculate cluster resource usage
    echo "--- Cluster Resource Summary ---"
    
    # Get total cluster capacity
    total_cpu=$(kubectl describe nodes | grep "cpu:" | grep "Capacity" | awk '{sum += $2} END {print sum}')
    total_memory=$(kubectl describe nodes | grep "memory:" | grep "Capacity" | awk '{sum += $2} END {print sum}')
    
    # Get allocated resources
    allocated_cpu=$(kubectl describe nodes | grep "cpu" | grep "Allocated resources" -A 10 | grep "cpu" | awk '{sum += $2} END {print sum}')
    allocated_memory=$(kubectl describe nodes | grep "memory" | grep "Allocated resources" -A 10 | grep "memory" | awk '{sum += $2} END {print sum}')
    
    echo "Total CPU: ${total_cpu}m"
    echo "Allocated CPU: ${allocated_cpu}m"
    echo "Total Memory: ${total_memory}Ki"
    echo "Allocated Memory: ${allocated_memory}Ki"
    
    # Check for resource pressure
    if command -v bc >/dev/null 2>&1; then
        cpu_usage=$(echo "scale=2; $allocated_cpu * 100 / $total_cpu" | bc 2>/dev/null || echo "N/A")
        echo "CPU Usage: ${cpu_usage}%"
    fi
}

# Test autoscaling behavior
test_autoscaling() {
    echo "=== Testing Autoscaling Behavior ==="
    
    # Create resource-intensive deployment
    echo "--- Creating test workload ---"
    cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: autoscaler-test
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: autoscaler-test
  template:
    metadata:
      labels:
        app: autoscaler-test
    spec:
      containers:
      - name: test
        image: busybox
        command: ["sleep", "3600"]
        resources:
          requests:
            cpu: 1000m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 2Gi
EOF

    # Wait for deployment
    kubectl wait --for=condition=available deployment/autoscaler-test --timeout=60s
    
    if [ $? -eq 0 ]; then
        echo "âœ… Test deployment created"
    else
        echo "âŒ Test deployment failed"
        return 1
    fi
    
    # Scale up the deployment to trigger autoscaling
    echo "--- Scaling up test deployment ---"
    kubectl scale deployment autoscaler-test --replicas=10
    
    # Monitor for a few minutes
    echo "--- Monitoring scaling behavior ---"
    for i in $(seq 1 12); do
        echo "Check $i/12:"
        kubectl get pods -l app=autoscaler-test --no-headers | wc -l | xargs echo "Running pods:"
        kubectl get pods -l app=autoscaler-test --field-selector=status.phase=Pending --no-headers | wc -l | xargs echo "Pending pods:"
        kubectl get nodes --no-headers | wc -l | xargs echo "Total nodes:"
        echo "---"
        sleep 30
    done
}

# Generate autoscaling report
generate_autoscaling_report() {
    echo "=== Autoscaling Report ==="
    
    # Cluster overview
    echo "--- Cluster Overview ---"
    echo "Cluster: $(kubectl config current-context)"
    echo "Nodes: $(kubectl get nodes --no-headers | wc -l)"
    echo "Pods: $(kubectl get pods --all-namespaces --no-headers | wc -l)"
    echo "Pending Pods: $(kubectl get pods --all-namespaces --field-selector=status.phase=Pending --no-headers | wc -l)"
    
    # Autoscaler configuration
    echo "--- Autoscaler Configuration ---"
    autoscaler_pod=$(kubectl get pods -n kube-system -l app=cluster-autoscaler -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
    if [ -n "$autoscaler_pod" ]; then
        kubectl describe pod $autoscaler_pod -n kube-system | grep -A 20 "Command:"
    fi
    
    # Recent scaling activity
    echo "--- Recent Scaling Activity ---"
    kubectl get events --all-namespaces --sort-by='.lastTimestamp' | grep -i "scale\|autoscaler" | tail -5
    
    # Resource utilization
    echo "--- Resource Utilization ---"
    kubectl top nodes 2>/dev/null || echo "Metrics server not available"
}

# Cleanup test resources
cleanup_test_resources() {
    echo "=== Cleaning up Test Resources ==="
    
    kubectl delete deployment autoscaler-test --ignore-not-found=true
    kubectl delete job load-generator --ignore-not-found=true
    
    echo "âœ… Cleanup completed"
}

# Main execution
main() {
    local action=${1:-"monitor"}
    
    case $action in
        "monitor")
            echo "Monitoring Cluster Autoscaler"
            echo ""
            
            monitor_autoscaler_status
            echo ""
            
            monitor_node_status
            echo ""
            
            monitor_pending_pods
            echo ""
            
            monitor_scaling_events
            echo ""
            
            monitor_resource_pressure
            ;;
            
        "test")
            echo "Testing Autoscaling Behavior"
            echo ""
            
            test_autoscaling
            echo ""
            
            read -p "Cleanup test resources? (y/n): " cleanup
            if [ "$cleanup" = "y" ]; then
                cleanup_test_resources
            fi
            ;;
            
        "report")
            echo "Generating Autoscaling Report"
            echo ""
            
            generate_autoscaling_report
            ;;
            
        *)
            echo "Usage: $0 [monitor|test|report]"
            echo ""
            echo "Commands:"
            echo "  monitor  - Monitor current autoscaler status"
            echo "  test     - Test autoscaling behavior"
            echo "  report   - Generate comprehensive report"
            ;;
    esac
}

# Execute main function
main "$@"
```

## ğŸ”„ **ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹:**

### **1. Scale-Up Decision:**
```bash
# Ğ£ÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°:
# 1. Pod'Ñ‹ Ğ² ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸ Pending > 10 ÑĞµĞºÑƒĞ½Ğ´
# 2. ĞĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Node'Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ
# 3. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Node'Ğ° Ñ€ĞµÑˆĞ¸Ñ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ
# 4. ĞĞµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ñ‹ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ñ‹ node pool

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° pending Pod'Ğ¾Ğ²
kubectl describe pod <pending-pod> | grep -A 10 "Events"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° resource requests
kubectl describe pod <pending-pod> | grep -A 5 "Requests"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° node capacity
kubectl describe nodes | grep -A 10 "Allocatable"
```

### **2. Scale-Down Decision:**
```bash
# Ğ£ÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°:
# 1. Node Ğ½ĞµĞ´Ğ¾Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ > 10 Ğ¼Ğ¸Ğ½ÑƒÑ‚
# 2. Ğ’ÑĞµ Pod'Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰ĞµĞ½Ñ‹
# 3. ĞĞµÑ‚ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ PodDisruptionBudget
# 4. ĞĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… Pod'Ğ¾Ğ² (ĞºÑ€Ğ¾Ğ¼Ğµ DaemonSet)

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Node'Ğ¾Ğ²
kubectl top nodes

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Pod'Ğ¾Ğ² Ğ½Ğ° Node'Ğ°Ñ…
kubectl describe node <node-name> | grep -A 20 "Non-terminated Pods"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° PodDisruptionBudgets
kubectl get pdb --all-namespaces
```

## ğŸ”§ **Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Auto-scaling:**

### **1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ´Ğ»Ñ scale-up:**
```bash
# Deployment Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼Ğ¸ resource requests
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource-hungry
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: resource-hungry
  template:
    metadata:
      labels:
        app: resource-hungry
    spec:
      containers:
      - name: app
        image: busybox
        command: ["sleep", "3600"]
        resources:
          requests:
            cpu: 1500m
            memory: 2Gi
          limits:
            cpu: 2000m
            memory: 3Gi
EOF

# ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ pressure
kubectl scale deployment resource-hungry --replicas=5

# ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ pending Pod'Ğ¾Ğ²
kubectl get pods -l app=resource-hungry -w
```

### **2. HPA Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ scaling:**
```bash
# HorizontalPodAutoscaler
cat << EOF | kubectl apply -f -
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: resource-hungry-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: resource-hungry
  minReplicas: 1
  maxReplicas: 15
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
EOF

# Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ CPU Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸
kubectl patch deployment resource-hungry -p '{"spec":{"template":{"spec":{"containers":[{"name":"app","command":["sh","-c","while true; do dd if=/dev/zero of=/dev/null; done"]}]}}}}'

# ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ HPA
kubectl get hpa resource-hungry-hpa -w
```

### **3. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ scale-down:**
```bash
# Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸
kubectl scale deployment resource-hungry --replicas=1

# ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° CPU Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸
kubectl patch deployment resource-hungry -p '{"spec":{"template":{"spec":{"containers":[{"name":"app","command":["sleep","3600"]}]}}}}'

# ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° (Ğ·Ğ°Ğ¹Ğ¼ĞµÑ‚ ~10-15 Ğ¼Ğ¸Ğ½ÑƒÑ‚)
watch "kubectl get nodes; echo '---'; kubectl top nodes"

# ĞÑ‡Ğ¸ÑÑ‚ĞºĞ°
kubectl delete deployment resource-hungry
kubectl delete hpa resource-hungry-hpa
```

## ğŸ“ˆ **ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Auto-scaling:**

### **1. DigitalOcean Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸:**
```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° node pool ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°
doctl kubernetes cluster node-pool get hashfoundry-ha ha-worker-pool

# Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ scaling ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ² DigitalOcean
doctl kubernetes cluster node-pool list hashfoundry-ha

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ¾Ğ² auto-scaling
doctl kubernetes cluster get hashfoundry-ha --format ID,Name,Status,AutoUpgrade,SurgeUpgrade,HA
```

### **2. Kubernetes ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ:**
```bash
# Ğ¡Ğ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Node'Ğ°Ğ¼Ğ¸
kubectl get events --all-namespaces --field-selector involvedObject.kind=Node

# Ğ¡Ğ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°
kubectl get events --all-namespaces --field-selector reason=FailedScheduling

# ĞĞ±Ñ‰Ğ¸Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
kubectl get events --all-namespaces --sort-by='.lastTimestamp' | tail -20
```

### **3. Prometheus Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸:**
```bash
# Port forward Ğº Prometheus
kubectl port-forward svc/prometheus-server -n monitoring 9090:80

# ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ auto-scaling:
# - kube_node_status_ready
# - kube_pod_status_phase{phase="Pending"}
# - node_cpu_utilization
# - node_memory_utilization
# - kube_deployment_status_replicas
```

## ğŸ­ **Auto-scaling Ğ² Ğ²Ğ°ÑˆĞµĞ¼ HA ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğµ:**

### **1. DigitalOcean Integration:**
```bash
# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ HA ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
kubectl get nodes -o custom-columns=NAME:.metadata.name,INSTANCE-TYPE:.metadata.labels.node\\.kubernetes\\.io/instance-type,ZONE:.metadata.labels.topology\\.kubernetes\\.io/zone

# Auto-scaling Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸
doctl kubernetes cluster node-pool get hashfoundry-ha ha-worker-pool --format Name,Size,Count,AutoScale,MinNodes,MaxNodes

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ·Ğ¾Ğ½Ğ°Ğ¼
kubectl get nodes --show-labels | grep topology.kubernetes.io/zone
```

### **2. ArgoCD Ğ¸ Auto-scaling:**
```bash
# ArgoCD Pod'Ñ‹ Ğ¸ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ
kubectl get pods -n argocd -o wide

# Resource requests ArgoCD
kubectl describe deployment argocd-server -n argocd | grep -A 10 "Requests"

# Ğ’Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ArgoCD Ğ½Ğ° auto-scaling
kubectl top pods -n argocd
```

### **3. Monitoring Stack Ğ¸ Auto-scaling:**
```bash
# Prometheus Ğ¸ Grafana Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ
kubectl get pods -n monitoring -o wide

# Resource usage monitoring stack
kubectl top pods -n monitoring

# PersistentVolumes Ğ¸ auto-scaling
kubectl get pv | grep monitoring
```

## ğŸš¨ **Troubleshooting Auto-scaling:**

### **1. Pod'Ñ‹ Ğ½Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ:**
```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° resource requests
kubectl describe pod <pending-pod> | grep -A 5 "Requests"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° node capacity
kubectl describe nodes | grep -A 10 "Allocatable"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° taints Ğ¸ tolerations
kubectl describe nodes | grep -A 5 "Taints"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° node selectors
kubectl describe pod <pending-pod> | grep -A 5 "Node-Selectors"
```

### **2. ĞšĞ»Ğ°ÑÑ‚ĞµÑ€ Ğ½Ğµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ÑÑ:**
```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° PodDisruptionBudgets
kubectl get pdb --all-namespaces

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… Pod'Ğ¾Ğ²
kubectl get pods --all-namespaces -o wide | grep -v "kube-system\|default"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° local storage
kubectl get pods --all-namespaces -o yaml | grep -A 5 "hostPath\|emptyDir"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° non-replicated Pod'Ğ¾Ğ²
kubectl get pods --all-namespaces --field-selector metadata.ownerReferences=null
```

### **3. ĞœĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:**
```bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° DigitalOcean API limits
doctl auth list

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Node'Ğ¾Ğ²
kubectl get events --all-namespaces | grep "Started\|Created" | grep node

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° image pull Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
kubectl describe pod <pod-name> | grep -A 10 "Events" | grep "Pulling\|Pulled"
```

## ğŸ¯ **ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Auto-scaling:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            DigitalOcean Auto-scaling Architecture          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Kubernetes Scheduler                                       â”‚
â”‚  â”œâ”€â”€ Detect pending pods                                   â”‚
â”‚  â”œâ”€â”€ Evaluate resource requirements                        â”‚
â”‚  â””â”€â”€ Trigger scaling decisions                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DigitalOcean Control Plane                                â”‚
â”‚  â”œâ”€â”€ Monitor node pool utilization                         â”‚
â”‚  â”œâ”€â”€ Apply auto-scaling policies                           â”‚
â”‚  â”œâ”€â”€ Manage min/max node limits                            â”‚
â”‚  â””â”€â”€ Coordinate with Kubernetes API                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Node Pool Management                                       â”‚
â”‚  â”œâ”€â”€ Scale-up: Add nodes when needed                       â”‚
â”‚  â”œâ”€â”€ Scale-down: Remove underutilized nodes                â”‚
â”‚  â”œâ”€â”€ Health checks and node replacement                    â”‚
â”‚  â””â”€â”€ Zone distribution and HA                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Resource Monitoring                                        â”‚
â”‚  â”œâ”€â”€ CPU and memory utilization                            â”‚
â”‚  â”œâ”€â”€ Pod scheduling success rate                           â”‚
â”‚  â”œâ”€â”€ Node availability and health                          â”‚
â”‚  â””â”€â”€ Application performance metrics                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ **Best Practices Ğ´Ğ»Ñ Auto-scaling:**

### **1. Resource Planning:**
- Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ resource requests
- ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ min/max Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ node pools
- Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ğ¹Ñ‚Ğµ startup Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹

### **2. Cost Optimization:**
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ instance types
- ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹Ñ‚Ğµ aggressive scale-down Ğ´Ğ»Ñ dev
- ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€ÑŒÑ‚Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ

### **3. High Availability:**
- Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞ¹Ñ‚Ğµ Node'Ñ‹ Ğ¿Ğ¾ Ğ·Ğ¾Ğ½Ğ°Ğ¼
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ PodDisruptionBudgets
- ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ capacity Ğ´Ğ»Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²

### **4. Monitoring Ğ¸ Alerting:**
- ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€ÑŒÑ‚Ğµ pending Pod'Ñ‹
- ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹Ñ‚Ğµ Ğ°Ğ»ĞµÑ€Ñ‚Ñ‹ Ğ½Ğ° scaling ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ
- ĞÑ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ¹Ñ‚Ğµ performance Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸

**Cluster Autoscaler â€” ÑÑ‚Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ² production Kubernetes ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°Ñ…!**
