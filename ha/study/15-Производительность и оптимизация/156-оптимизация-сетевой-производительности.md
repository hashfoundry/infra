# 156. Как оптимизировать сетевую производительность?

## 🎯 Вопрос
Как оптимизировать сетевую производительность?

## 💡 Ответ

Оптимизация сетевой производительности в Kubernetes включает настройку CNI, kube-proxy, DNS, балансировщиков нагрузки и сетевых политик для достижения минимальной задержки и максимальной пропускной способности.

### 🏗️ Архитектура сетевой производительности

#### 1. **Схема сетевых компонентов**
```
┌─────────────────────────────────────────────────────────────┐
│                Network Performance Stack                   │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │
│  │     CNI     │  │ kube-proxy  │  │    DNS      │         │
│  │  (Calico)   │  │  (iptables) │  │ (CoreDNS)   │         │
│  └─────────────┘  └─────────────┘  └─────────────┘         │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │
│  │   Ingress   │  │   Service   │  │  Network    │         │
│  │    NGINX    │  │ LoadBalancer│  │  Policies   │         │
│  └─────────────┘  └─────────────┘  └─────────────┘         │
└─────────────────────────────────────────────────────────────┘
```

#### 2. **Ключевые метрики производительности**
```yaml
# Сетевые метрики производительности
network_performance_metrics:
  latency_metrics:
    - "container_network_receive_packets_total"
    - "container_network_transmit_packets_total"
    - "node_network_receive_bytes_total"
    - "node_network_transmit_bytes_total"
  
  throughput_metrics:
    - "rate(container_network_receive_bytes_total[5m])"
    - "rate(container_network_transmit_bytes_total[5m])"
    - "nginx_ingress_controller_request_duration_seconds"
    - "coredns_dns_request_duration_seconds"
  
  error_metrics:
    - "container_network_receive_errors_total"
    - "container_network_transmit_errors_total"
    - "coredns_dns_request_count_total"
    - "nginx_ingress_controller_requests"
```

### 📊 Примеры из нашего кластера

#### Проверка сетевой производительности:
```bash
# Информация о сетевых интерфейсах
kubectl get nodes -o wide

# Проверка CNI подов
kubectl get pods -n kube-system | grep calico

# Статистика сетевого трафика
kubectl top pods --all-namespaces --sort-by=memory
```

### 🌐 Оптимизация CNI

#### 1. **Настройка Calico для производительности**
```yaml
# calico-performance-config.yaml
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  calicoNetwork:
    # Оптимизация производительности
    mtu: 1500                             # Оптимальный MTU
    nodeAddressAutodetectionV4:
      interface: "eth0"                   # Основной интерфейс
    ipPools:
    - blockSize: 26                       # Оптимальный размер блока
      cidr: 10.244.0.0/16
      encapsulation: VXLAN                # VXLAN для производительности
      natOutgoing: Enabled
      nodeSelector: all()
  
  # Настройки производительности
  nodeMetricsPort: 9091
  typhaMetricsPort: 9093
  
  # Ресурсы для компонентов
  calicoNodeDaemonSet:
    spec:
      template:
        spec:
          containers:
          - name: calico-node
            resources:
              requests:
                cpu: 250m
                memory: 512Mi
              limits:
                cpu: 500m
                memory: 1Gi
---
# Felix конфигурация для производительности
apiVersion: projectcalico.org/v3
kind: FelixConfiguration
metadata:
  name: default
spec:
  # Оптимизация производительности
  bpfLogLevel: "Off"                      # Отключение BPF логов
  chainInsertMode: "Insert"               # Быстрая вставка правил
  dataplaneDriver: "iptables"             # Драйвер dataplane
  defaultEndpointToHostAction: "ACCEPT"   # Действие по умолчанию
  deviceRouteSourceAddress: "Use"         # Использование source address
  iptablesBackend: "Auto"                 # Автоматический backend
  iptablesLockTimeout: "10s"              # Таймаут блокировки
  iptablesPostWriteCheckInterval: "1s"    # Интервал проверки
  iptablesRefreshInterval: "60s"          # Интервал обновления
  logSeverityScreen: "Info"               # Уровень логирования
  prometheusMetricsEnabled: true          # Метрики Prometheus
  reportingInterval: "30s"                # Интервал отчетности
  
  # Оптимизация маршрутизации
  routeRefreshInterval: "90s"             # Интервал обновления маршрутов
  routeTableRange: "1-250"                # Диапазон таблиц маршрутизации
```

#### 2. **Скрипт оптимизации CNI**
```bash
#!/bin/bash
# cni-optimization.sh

echo "🌐 Оптимизация CNI производительности"

# Проверка текущего MTU
echo "📊 Текущий MTU:"
kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' | \
xargs -I {} sh -c 'echo "Node {}: $(ip link show | grep mtu | head -1)"'

# Оптимизация Calico Felix
echo "🔧 Применение оптимизаций Felix..."
kubectl patch felixconfiguration default --patch='
spec:
  bpfLogLevel: "Off"
  chainInsertMode: "Insert"
  iptablesRefreshInterval: "60s"
  reportingInterval: "30s"
  routeRefreshInterval: "90s"
'

# Проверка статуса Calico
echo "✅ Статус Calico компонентов:"
kubectl get pods -n calico-system -o wide

# Проверка производительности сети
echo "📈 Тестирование сетевой производительности..."
kubectl run netperf-client --image=networkstatic/netperf --rm -it -- netperf -H calico-node-service

echo "✅ Оптимизация CNI завершена"
```

### ⚖️ Оптимизация kube-proxy

#### 1. **Настройка kube-proxy для производительности**
```yaml
# kube-proxy-performance-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy
  namespace: kube-system
data:
  config.conf: |
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    kind: KubeProxyConfiguration
    # Режим работы
    mode: "iptables"                      # iptables или ipvs
    
    # Настройки iptables
    iptables:
      masqueradeAll: false                # Не маскировать весь трафик
      masqueradeBit: 14                   # Бит для маскировки
      minSyncPeriod: 1s                   # Минимальный период синхронизации
      syncPeriod: 30s                     # Период синхронизации
    
    # Настройки IPVS (если используется)
    ipvs:
      minSyncPeriod: 5s                   # Минимальный период для IPVS
      syncPeriod: 30s                     # Период синхронизации IPVS
      scheduler: "rr"                     # Round-robin планировщик
      excludeCIDRs: []                    # Исключенные CIDR
      strictARP: false                    # Строгий ARP
    
    # Общие настройки
    clusterCIDR: "10.244.0.0/16"          # CIDR кластера
    hostnameOverride: ""                  # Переопределение hostname
    bindAddress: "0.0.0.0"                # Адрес привязки
    healthzBindAddress: "0.0.0.0:10256"   # Адрес healthz
    metricsBindAddress: "127.0.0.1:10249" # Адрес метрик
    
    # Настройки производительности
    oomScoreAdj: -999                     # OOM score adjustment
    resourceContainer: "/kube-proxy"      # Контейнер ресурсов
    udpIdleTimeout: 250ms                 # Таймаут UDP
    conntrack:
      maxPerCore: 32768                   # Максимум conntrack на ядро
      min: 131072                         # Минимум conntrack
      tcpEstablishedTimeout: 86400s       # Таймаут TCP established
      tcpCloseWaitTimeout: 3600s          # Таймаут TCP close wait
```

#### 2. **Скрипт оптимизации kube-proxy**
```bash
#!/bin/bash
# kube-proxy-optimization.sh

echo "⚖️ Оптимизация kube-proxy"

# Проверка текущего режима kube-proxy
echo "📊 Текущий режим kube-proxy:"
kubectl get configmap kube-proxy -n kube-system -o yaml | grep mode

# Применение оптимизированной конфигурации
echo "🔧 Применение оптимизаций..."
kubectl patch configmap kube-proxy -n kube-system --patch='
data:
  config.conf: |
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    kind: KubeProxyConfiguration
    mode: "iptables"
    iptables:
      masqueradeAll: false
      masqueradeBit: 14
      minSyncPeriod: 1s
      syncPeriod: 30s
    clusterCIDR: "10.244.0.0/16"
    oomScoreAdj: -999
    udpIdleTimeout: 250ms
    conntrack:
      maxPerCore: 32768
      min: 131072
'

# Перезапуск kube-proxy подов
echo "🔄 Перезапуск kube-proxy..."
kubectl rollout restart daemonset kube-proxy -n kube-system

# Проверка статуса
echo "✅ Статус kube-proxy:"
kubectl get pods -n kube-system -l k8s-app=kube-proxy

echo "✅ Оптимизация kube-proxy завершена"
```

### 🔍 Оптимизация DNS

#### 1. **Настройка CoreDNS для производительности**
```yaml
# coredns-performance-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
            lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
            ttl 30                        # TTL для кэширования
        }
        prometheus :9153
        forward . /etc/resolv.conf {
            max_concurrent 1000           # Максимум concurrent запросов
        }
        cache 30 {                        # Кэширование на 30 секунд
            success 9984 30               # Кэш успешных запросов
            denial 9984 5                 # Кэш отказов
        }
        loop
        reload
        loadbalance                       # Балансировка нагрузки
    }
---
# Deployment CoreDNS с оптимизациями
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
spec:
  replicas: 3                             # Увеличенное количество реплик
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      containers:
      - name: coredns
        image: coredns/coredns:1.10.1
        resources:
          requests:
            cpu: 100m
            memory: 70Mi
          limits:
            cpu: 200m                     # Увеличенные лимиты CPU
            memory: 170Mi                 # Увеличенные лимиты памяти
        args: [ "-conf", "/etc/coredns/Corefile" ]
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
          initialDelaySeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
```

#### 2. **Скрипт оптимизации DNS**
```bash
#!/bin/bash
# dns-optimization.sh

echo "🔍 Оптимизация DNS производительности"

# Проверка текущей конфигурации CoreDNS
echo "📊 Текущая конфигурация CoreDNS:"
kubectl get configmap coredns -n kube-system -o yaml | grep -A 20 Corefile

# Применение оптимизированной конфигурации
echo "🔧 Применение DNS оптимизаций..."
kubectl patch configmap coredns -n kube-system --patch='
data:
  Corefile: |
    .:53 {
        errors
        health {
            lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
            ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
            max_concurrent 1000
        }
        cache 30 {
            success 9984 30
            denial 9984 5
        }
        loop
        reload
        loadbalance
    }
'

# Масштабирование CoreDNS
echo "📈 Масштабирование CoreDNS..."
kubectl scale deployment coredns -n kube-system --replicas=3

# Тестирование DNS производительности
echo "🧪 Тестирование DNS..."
kubectl run dns-test --image=busybox --rm -it -- nslookup kubernetes.default.svc.cluster.local

echo "✅ Оптимизация DNS завершена"
```

### 🚪 Оптимизация Ingress

#### 1. **Настройка NGINX Ingress для производительности**
```yaml
# nginx-ingress-performance.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
data:
  # Основные настройки производительности
  worker-processes: "auto"                # Автоматическое определение процессов
  worker-connections: "16384"             # Увеличенные соединения
  worker-rlimit-nofile: "65536"           # Лимит файловых дескрипторов
  
  # Настройки буферизации
  proxy-buffer-size: "16k"                # Размер буфера proxy
  proxy-buffers-number: "8"               # Количество буферов
  proxy-busy-buffers-size: "64k"          # Размер busy буферов
  
  # Настройки таймаутов
  proxy-connect-timeout: "5"              # Таймаут подключения
  proxy-send-timeout: "60"                # Таймаут отправки
  proxy-read-timeout: "60"                # Таймаут чтения
  
  # Настройки keep-alive
  upstream-keepalive-connections: "100"   # Keep-alive соединения
  upstream-keepalive-requests: "1000"     # Keep-alive запросы
  upstream-keepalive-timeout: "60"        # Keep-alive таймаут
  
  # Настройки сжатия
  enable-gzip: "true"                     # Включение gzip
  gzip-level: "6"                         # Уровень сжатия
  gzip-types: "text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript"
  
  # Настройки SSL
  ssl-protocols: "TLSv1.2 TLSv1.3"       # Протоколы SSL
  ssl-ciphers: "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384"
  ssl-session-cache: "shared:SSL:10m"     # Кэш SSL сессий
  ssl-session-timeout: "10m"              # Таймаут SSL сессий
  
  # Настройки логирования
  access-log-path: "/var/log/nginx/access.log"
  error-log-path: "/var/log/nginx/error.log"
  log-format-upstream: '$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" $request_length $request_time [$proxy_upstream_name] [$proxy_alternative_upstream_name] $upstream_addr $upstream_response_length $upstream_response_time $upstream_status $req_id'
---
# Deployment NGINX Ingress с оптимизациями
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
spec:
  replicas: 3                             # Множественные реплики
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - name: controller
        image: k8s.gcr.io/ingress-nginx/controller:v1.8.1
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 500m                     # Увеличенные лимиты
            memory: 512Mi
        args:
        - /nginx-ingress-controller
        - --configmap=$(POD_NAMESPACE)/nginx-configuration
        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx
        - --annotations-prefix=nginx.ingress.kubernetes.io
        - --enable-ssl-passthrough
        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        ports:
        - name: http
          containerPort: 80
        - name: https
          containerPort: 443
        - name: metrics
          containerPort: 10254
```

### 📊 Мониторинг сетевой производительности

#### 1. **Prometheus метрики для сети**
```yaml
# network-monitoring.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: network-performance
spec:
  selector:
    matchLabels:
      app: network-monitoring
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics
---
# Ключевые сетевые метрики
network_key_metrics:
  bandwidth:
    - "rate(container_network_receive_bytes_total[5m])"
    - "rate(container_network_transmit_bytes_total[5m])"
    - "rate(node_network_receive_bytes_total[5m])"
    - "rate(node_network_transmit_bytes_total[5m])"
  
  latency:
    - "histogram_quantile(0.95, nginx_ingress_controller_request_duration_seconds_bucket)"
    - "histogram_quantile(0.95, coredns_dns_request_duration_seconds_bucket)"
    - "histogram_quantile(0.95, prometheus_rule_evaluation_duration_seconds_bucket)"
  
  errors:
    - "rate(container_network_receive_errors_total[5m])"
    - "rate(container_network_transmit_errors_total[5m])"
    - "rate(coredns_dns_request_count_total{rcode!=\"NOERROR\"}[5m])"
    - "rate(nginx_ingress_controller_requests{status=~\"5..\"}[5m])"
```

#### 2. **Скрипт мониторинга сети**
```bash
#!/bin/bash
# network-performance-monitoring.sh

echo "📊 Мониторинг сетевой производительности"

# Проверка пропускной способности
echo "🌐 Пропускная способность сети:"
kubectl top nodes --sort-by=memory

# Проверка DNS производительности
echo -e "\n🔍 Производительность DNS:"
kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide

# Проверка Ingress производительности
echo -e "\n🚪 Производительность Ingress:"
kubectl get pods -n ingress-nginx -o wide

# Тестирование сетевой задержки
echo -e "\n⏱️ Тестирование задержки:"
kubectl run nettest --image=busybox --rm -it -- ping -c 3 8.8.8.8

# Проверка сетевых политик
echo -e "\n🛡️ Сетевые политики:"
kubectl get networkpolicies --all-namespaces

echo "✅ Мониторинг завершен"
```

### 🎯 Лучшие практики

#### 1. **Чек-лист сетевой оптимизации**
```yaml
network_optimization_checklist:
  cni:
    - "✅ Оптимизирован MTU для сети"
    - "✅ Настроен правильный encapsulation"
    - "✅ Felix конфигурация оптимизирована"
    - "✅ Мониторинг CNI метрик настроен"
  
  kube_proxy:
    - "✅ Выбран оптимальный режим (iptables/ipvs)"
    - "✅ Настроены таймауты и периоды синхронизации"
    - "✅ Оптимизированы conntrack настройки"
    - "✅ Мониторинг kube-proxy настроен"
  
  dns:
    - "✅ CoreDNS масштабирован правильно"
    - "✅ Настроено кэширование DNS"
    - "✅ Оптимизированы TTL значения"
    - "✅ Мониторинг DNS производительности"
  
  ingress:
    - "✅ NGINX Ingress оптимизирован"
    - "✅ Настроена буферизация и keep-alive"
    - "✅ Включено сжатие gzip"
    - "✅ SSL оптимизирован"
  
  monitoring:
    - "✅ Сетевые метрики собираются"
    - "✅ Алерты на сетевые проблемы настроены"
    - "✅ Дашборды производительности созданы"
    - "✅ Регулярное тестирование производительности"
```

Правильная оптимизация сетевой производительности критически важна для обеспечения быстрой и надежной работы приложений в Kubernetes кластере.
