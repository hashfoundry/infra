kube-prometheus-stack:
  # Global settings
  global:
    imageRegistry: ""
    imagePullSecrets: []
  
  # Prometheus configuration
  prometheus:
    enabled: true
    
    prometheusSpec:
      # Image settings
      image:
        repository: quay.io/prometheus/prometheus
        tag: v2.45.0
      
      # Resource limits
      resources:
        limits:
          cpu: 1000m
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 1Gi
      
      # Storage configuration
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: do-block-storage
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 20Gi
      
      # Retention
      retention: 30d
      retentionSize: 18GB
      
      # Security context
      securityContext:
        runAsUser: 1000
        runAsGroup: 2000
        fsGroup: 2000
      
      # Service Monitor selector (enable all namespaces)
      serviceMonitorSelectorNilUsesHelmValues: false
      serviceMonitorSelector: {}
      serviceMonitorNamespaceSelector: {}
      
      # Pod Monitor selector
      podMonitorSelectorNilUsesHelmValues: false
      podMonitorSelector: {}
      podMonitorNamespaceSelector: {}
      
      # Rule selector
      ruleSelectorNilUsesHelmValues: false
      ruleSelector: {}
      ruleNamespaceSelector: {}
      
      # Anti-affinity for HA
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: prometheus
              topologyKey: kubernetes.io/hostname
      
      # Additional scrape configs
      additionalScrapeConfigs:
        - job_name: 'argocd-metrics'
          static_configs:
            - targets: ['argocd-metrics.argocd.svc.cluster.local:8082']
        
        - job_name: 'argocd-server-metrics'
          static_configs:
            - targets: ['argocd-server-metrics.argocd.svc.cluster.local:8083']
        
        - job_name: 'argocd-repo-server-metrics'
          static_configs:
            - targets: ['argocd-repo-server.argocd.svc.cluster.local:8084']
    
    # Service configuration
    service:
      type: ClusterIP
      port: 9090
    
    # Ingress configuration
    ingress:
      enabled: true
      ingressClassName: nginx
      hosts:
        - prometheus.hashfoundry.local
      tls:
        - secretName: prometheus-tls
          hosts:
            - prometheus.hashfoundry.local

  # Alertmanager configuration
  alertmanager:
    enabled: false
    
  # Grafana configuration (disable as we have separate Grafana)
  grafana:
    enabled: false
    
  # Node Exporter
  nodeExporter:
    enabled: true
    
  # Kube State Metrics
  kubeStateMetrics:
    enabled: true
    
  # Prometheus Operator
  prometheusOperator:
    enabled: true
    
    # Admission webhooks (disable for simplicity)
    admissionWebhooks:
      enabled: false
      patch:
        enabled: false
    
    # TLS configuration
    tls:
      enabled: false
    
    # ServiceMonitor for operator itself
    serviceMonitor:
      enabled: true
  
  # Default rules
  defaultRules:
    create: true
    rules:
      alertmanager: false
      etcd: true
      configReloaders: true
      general: true
      k8s: true
      kubeApiserver: true
      kubeApiserverAvailability: true
      kubeApiserverSlos: true
      kubelet: true
      kubeProxy: true
      kubePrometheusGeneral: true
      kubePrometheusNodeRecording: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      network: true
      node: true
      nodeExporterAlerting: true
      nodeExporterRecording: true
      prometheus: true
      prometheusOperator: true

# Additional ServiceMonitors and PrometheusRules will be created as separate resources
# This approach allows for better modularity and GitOps management

# ServiceMonitor for NFS Exporter (example)
serviceMonitors:
  nfs-exporter:
    enabled: true
    namespace: monitoring
    labels:
      app.kubernetes.io/name: nfs-exporter
    spec:
      selector:
        matchLabels:
          app.kubernetes.io/name: nfs-exporter
      endpoints:
      - port: metrics
        path: /metrics
        interval: 30s
        scrapeTimeout: 10s
        honorLabels: true
        relabelings:
        - sourceLabels: [__meta_kubernetes_pod_name]
          targetLabel: instance
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: node
        - sourceLabels: [__meta_kubernetes_namespace]
          targetLabel: kubernetes_namespace
        - sourceLabels: [__meta_kubernetes_service_name]
          targetLabel: kubernetes_name
        - targetLabel: job
          replacement: nfs-exporter

# PrometheusRules for custom alerting
prometheusRules:
  hashfoundry-alerts:
    enabled: true
    namespace: monitoring
    labels:
      app.kubernetes.io/name: hashfoundry-rules
    spec:
      groups:
        - name: argocd-alerts
          rules:
            - alert: ArgoCDAppNotSynced
              expr: argocd_app_info{sync_status!="Synced"} == 1
              for: 15m
              labels:
                severity: warning
              annotations:
                summary: ArgoCD application not synced
                description: "Application {{ $labels.name }} in project {{ $labels.project }} is not synced."
            
            - alert: ArgoCDAppUnhealthy
              expr: argocd_app_info{health_status!="Healthy"} == 1
              for: 15m
              labels:
                severity: critical
              annotations:
                summary: ArgoCD application unhealthy
                description: "Application {{ $labels.name }} in project {{ $labels.project }} is unhealthy."
        
        - name: nfs-alerts
          rules:
            - alert: NFSExporterDown
              expr: up{job="nfs-exporter"} == 0
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: NFS Exporter is down
                description: "NFS Exporter has been down for more than 5 minutes."
            
            - alert: FilesystemSpaceUsageHigh
              expr: (node_filesystem_size_bytes{job="nfs-exporter"} - node_filesystem_avail_bytes{job="nfs-exporter"}) / node_filesystem_size_bytes{job="nfs-exporter"} * 100 > 80
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: Filesystem space usage is high
                description: "Filesystem {{ $labels.mountpoint }} on {{ $labels.instance }} is {{ $value }}% full."
            
            - alert: FilesystemSpaceUsageCritical
              expr: (node_filesystem_size_bytes{job="nfs-exporter"} - node_filesystem_avail_bytes{job="nfs-exporter"}) / node_filesystem_size_bytes{job="nfs-exporter"} * 100 > 90
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: Filesystem space usage is critical
                description: "Filesystem {{ $labels.mountpoint }} on {{ $labels.instance }} is {{ $value }}% full."
